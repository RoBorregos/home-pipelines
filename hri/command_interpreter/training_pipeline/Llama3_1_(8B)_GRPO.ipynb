{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/RoBorregos/home-pipelines/blob/llm-train/Llama3_1_(8B)_GRPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLfKdDrOF4te"
   },
   "source": [
    "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
    "</div>\n",
    "\n",
    "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xySn35a1F4tg"
   },
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDBBfKAwF4th"
   },
   "source": [
    "**Read our [blog post](https://unsloth.ai/blog/r1-reasoning) for guidance on how to train reasoning models.**\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rStT4RY1F4th"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJrY28yPF4th"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Skip restarting message in Colab\n",
    "import sys; modules = list(sys.modules.keys())\n",
    "for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
    "\n",
    "!pip install unsloth==2025.3.6 unsloth_zoo==2025.3.4\n",
    "!pip install vllm\n",
    "!pip install --upgrade pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bmiQj2tF4ti"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1zyu9Ug2XEt"
   },
   "source": [
    "Use `PatchFastRL` before all functions to patch GRPO and other RL algorithms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59DIs5BMcvjN",
    "outputId": "d9a00109-39a5-4a91-c9bd-519713fe5bd5"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, PatchFastRL\n",
    "PatchFastRL(\"GRPO\", FastLanguageModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8-SLRUB2gwM"
   },
   "source": [
    "Load up `Llama 3.1 8B Instruct`, and set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 965,
     "referenced_widgets": [
      "2196b1b681884dd9aeee6438a46bc0b4",
      "52c9c83f1ebf48e3a467086a38f4cf2b",
      "7a5e2033a9014dcdb36bcc1d28e81896",
      "f5fa6dbd4c564eeda4b409551998364b",
      "100e31b4b5244a1baf5b8eec3feb79b0",
      "34a3a90150b7440a812b973c69737b40",
      "b92e3c46508d41d8b52376f140ba21e1",
      "0f4ccdb449554d33b0922fdebbb114ed",
      "f00d9c93f0544994ad5378060aac6360",
      "dd8c6b79aaa9416ca0154d23606491a0",
      "32312f7727674890af05461e34ebc110",
      "1411457e387c4324a7dff0869af49d2a",
      "b27a1ba063e446e496d146bf3047eac7",
      "8dc656076bdd4e14a6576ab924af0ee2",
      "cad7a3bdf8854e3fb686a0fd4ae82553",
      "59041105715947caa05a0f985cb86c2f",
      "4d5b3b551a1448ada40a00679e5b5b46",
      "456fddd8731a4c8797102730ac782685",
      "e8c20fee501c4282a8d7d36498f4bc44",
      "d995783415524e8d8da1e1158c5f4124",
      "acb3dd36e2ba41e9b2b68fe8a4747209",
      "c51abf6d21fe48a6a401efb8f77c71e7",
      "9231eca21c4f4eaa8545cff5cb90b646",
      "5f7223292bb144a4bcbab468ab36ecb1",
      "14426420ebfa4225a12a000b1958e2a1",
      "f833da31890648ff806400588453bcb7",
      "fb57436f8fca4265a2ba1aa43458bc18",
      "5b94588a042f40258ffcc5dec4ee5038",
      "d2a157b6376b4bcf89b6fd5b50d20e8d",
      "b464b30c24da42c1ad6430f60e92a8d8",
      "f58d5d9d14e54ef28cac1c9690ef7777",
      "5ca39ed7b8e044a6ae5b580df9ffbc62",
      "e8e54ad50d234568bdab2e03645245cb",
      "1ae036ac37f54ee3b908ae225e1f9f50",
      "3af6e21bf89445f3a7f84cb76aef8a28",
      "17c506934a93457095c03cf5a2dbcb4e",
      "3509fe6af07747a8a1eb81a59d31ff32",
      "f156e8495ab54f22a740b6df4af74c00",
      "82d1d4b60cea4b91b7fd768951132645",
      "64eb84b9b33d4a1eaf9ddc88a7204004",
      "b3099cca387f4e22bfad57b829f33194",
      "53c9b8352e5f47849483ecb47bc712b7",
      "0737266cfda940689b992c84b98ae0b2",
      "919c2e18b8374182a9d89cd7f0bb7663",
      "08e21e633ede45ce9969a4ae167eba6c",
      "85cd6dd7aa044ae5a01bf9ad65e286b8",
      "75f99cb34a054fbcb47b333c772d005b",
      "4baec3bdd68b4a09af93924ca04a0f7a",
      "d64756a8c83e4359b11c0acccfcf98a1",
      "ace36ffb3173493fb5bb98d1ba03a3a0",
      "19f8f1eff5e34245a8c025b5d7e239de",
      "1f63c388d3f4490c8529bdb75f60f808",
      "8d01f09ef12443c4b4c75f3b54936f35",
      "ceb62d236654487c8bed1742bf42b636",
      "7d17d0fdfdde4e21aafb2bf849b77d20",
      "2a8adf48301848559c37c393050ce593",
      "f4790a3644274a7d9a3bbc5142b7bfac",
      "75c8366fd8864e1c94db0d4fb6356b14",
      "36437353452949779c717543a5a904fb",
      "b0ba5c3e60f64b25aba70aa304ee85f4",
      "eb91fc4bcf4c483caa62f180fae04aa6",
      "41b733f823a24869aaa4b4067dd4fcd7",
      "8da6f18c651f42bb914c97cd10d01b5f",
      "b4e3302a8271416da2a4512ca5ecf6ee",
      "b888d72977244717b095904f9ee94030",
      "827d545d1002485ba1fa52d99513148e",
      "ca8bf8ccf34747b18a456f750398b959",
      "22ddd8b513dd450d9008c9b6af146d2d",
      "b4156daa5c714dab9f0ab9cf38b6015a",
      "adc69503eebc47f796c6a3eea77d50a0",
      "92ee8a352822474e86601e6fbbf58d85",
      "cff4e7a3574d4a88b2e0b5d6dd22da9f",
      "2116ad974e8e4ad69a81da40269c0c08",
      "d53a8b62dcce4e57840c6a495f34edc6",
      "7663f0f33d4942f7855a81cff7a696d4",
      "9c4791076cdf4d2a8ac0f4e9835f283a",
      "6d67b90ed14e48ea995a2689f8684a1a",
      "56b2816db84643cb99f414d30553e2fb",
      "aac3093a84f24358bd72a9d5ffb83b62",
      "ba110765c6684d86830d908aeb500664",
      "6c5ebaacc01a4b0682e6254dba375098",
      "e00e2f2ddc70417081de866d072ce60a",
      "92e19e9a2248498a9cf7719d1c1ee07f",
      "7d6a8d3035c04d98b4d4c09571eeb498",
      "a745f39269c64d46b0c492e7c3d3f3bf",
      "4aa13b614c114e6aae4490fbf40ef3b0",
      "83a09e5f04ec4579847a20db485456eb",
      "aec9fcf4642f4945a2b44f947f1e067b",
      "e03c6f7b1c0a4c7ba09d63cdaf2b2540",
      "37fc2dcf76bf4c229d236d1cf29baddf",
      "bf2e70e3d464460da088d32bf43e4159",
      "ad4fb67d414c4948a02bbaa9aead3d8c",
      "b29f71cb5ac3451b91d4142bd7353b56",
      "717be074484044548321c793afa53b26",
      "d571c7a53cd842589a88aaee1a5359dd",
      "08b002b81192408c895b90b0b7e6c2b9",
      "3dc0d6518eb74c48903e614b35c5d849",
      "66e36866b63f451f89fe09141d804e5f",
      "09addf5cf6ae40efaa85452f76dd6bf3",
      "c8ebb7cf2ba546b59418b1e04f66f2fb",
      "bc64fd796457468b9b501bf520c4febd",
      "95655d1bb45b43838c86bcebaa53a32b",
      "e05bb79c1bf24acfafbae39ff09520fe",
      "75883144ee134a748a268f04e80b03d7",
      "b0365ac78ad6411fbfa3313433b5476a",
      "776188cab1e64a4d876d04dae1ca6a5b",
      "237c0ea649a548e1a6d293568c683295",
      "164bf85d4371497898475cf783b163af",
      "34b89d0b496f4599bd776bf4290b555c",
      "1ffe7545ba234b19a8bce138471f3628"
     ]
    },
    "id": "DkIvEkIIkEyB",
    "outputId": "a54fd902-1f55-48cd-8600-c3e6f40afa7e"
   },
   "outputs": [],
   "source": [
    "from unsloth import is_bfloat16_supported\n",
    "import torch\n",
    "max_seq_length = 512 # Can increase for longer reasoning traces\n",
    "lora_rank = 32 # Larger rank = smarter, but slower\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True, # False for LoRA 16bit\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.6, # Reduce if out of memory\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ], # Remove QKVO if out of memory\n",
    "    lora_alpha = lora_rank,\n",
    "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KGgPgk_5S8r"
   },
   "source": [
    "### Data Prep\n",
    "<a name=\"Data\"></a>\n",
    "\n",
    "We directly leverage [@willccbb](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb) for data prep and all reward functions. You are free to create your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXk993X6C2ZZ"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from datasets import load_dataset, Dataset\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Load and prep dataset\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "XML_COT_FORMAT = \"\"\"\\\n",
    "<reasoning>\n",
    "{reasoning}\n",
    "</reasoning>\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "# uncomment middle messages for 1-shot prompting\n",
    "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
    "    data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore\n",
    "    data = data.map(lambda x: { # type: ignore\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': x['question']}\n",
    "        ],\n",
    "        'answer': extract_hash_answer(x['answer'])\n",
    "    }) # type: ignore\n",
    "    return data # type: ignore\n",
    "\n",
    "import json\n",
    "\n",
    "def load_custom_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Load custom dataset from JSON file and format for GRPO training\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        # Each line is a separate JSON object\n",
    "        data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "    # Create dataset dictionary with the format expected by GRPO trainer\n",
    "    dataset_dict = {\n",
    "        'prompt': [],\n",
    "        'answer': []\n",
    "    }\n",
    "\n",
    "    for item in data:\n",
    "        messages = item['messages']\n",
    "        # The prompt is all messages except the last one (assistant's response)\n",
    "        dataset_dict['prompt'].append(messages[:-1])\n",
    "        # The answer is the content of the assistant's message\n",
    "        dataset_dict['answer'].append(messages[-1]['content'])\n",
    "\n",
    "    # Convert to HuggingFace dataset\n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "    return dataset\n",
    "\n",
    "dataset = get_gsm8k_questions()\n",
    "# dataset = load_custom_dataset()\n",
    "\n",
    "# Define expected response structure\n",
    "class CommandShape(BaseModel):\n",
    "    action: str = Field(description=\"The action to be performed\")\n",
    "    characteristic: Optional[str] = Field(description=\"A characteristic related to the action\")\n",
    "    complement: Optional[str] = Field(description=\"A complement related to the action\")\n",
    "\n",
    "class CommandListShape(BaseModel):\n",
    "    commands: List[CommandShape]\n",
    "\n",
    "available_actions = [\n",
    "    \"go\", \"find\", \"pick\", \"give\", \"hear\", \"contextual_say\", \"count\",\n",
    "    \"follow_person_until\", \"escort_to\", \"follow_person_until_canceled\"\n",
    "]\n",
    "\n",
    "def verify_response_shape(response: str) -> bool:\n",
    "    \"\"\"Check if response is a valid CommandListShape.\"\"\"\n",
    "    try:\n",
    "        parsed = CommandListShape.model_validate_json(response)\n",
    "        return isinstance(parsed, CommandListShape)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def verify_soft_response_shape(response: str) -> bool:\n",
    "    \"\"\"Check if response contains something resembling commands.\"\"\"\n",
    "    return \"commands\" in response and \"action\" in response\n",
    "\n",
    "def has_valid_actions(response: str) -> bool:\n",
    "    \"\"\"Check if the response contains only available actions.\"\"\"\n",
    "    try:\n",
    "        parsed = CommandListShape.model_validate_json(response)\n",
    "        for cmd in parsed.commands:\n",
    "            if cmd.action not in available_actions:\n",
    "                return False\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def has_characteristics_and_complements(response: str) -> bool:\n",
    "    \"\"\"Check if complement is present but characteristic is not\"\"\"\n",
    "    try:\n",
    "        parsed = CommandListShape.model_validate_json(response)\n",
    "        for cmd in parsed.commands:\n",
    "            if cmd.complement is not None and cmd.characteristic is None:\n",
    "                return False\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Combined reward functions\n",
    "def response_shape_reward_func(completions, **kwargs) -> list[float]:\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [1.0 if verify_response_shape(c) else 0.0 for c in contents]\n",
    "\n",
    "def soft_response_shape_reward_func(completions, **kwargs) -> list[float]:\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [0.5 if verify_soft_response_shape(c) else 0.0 for c in contents]\n",
    "\n",
    "def valid_action_reward_func(completions, **kwargs) -> list[float]:\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [1.0 if has_valid_actions(c) else 0.0 for c in contents]\n",
    "\n",
    "def characteristics_complements_reward_func(completions, **kwargs) -> list[float]:\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [1.0 if has_characteristics_and_complements(c) else 0.0 for c in contents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ux6iqP7z5YOo"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "\n",
    "Now set up GRPO Trainer and all configurations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptqkXK2D4d6p"
   },
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    use_vllm = True, # use vLLM for fast inference!\n",
    "    learning_rate = 5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    bf16 = is_bfloat16_supported(),\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
    "    num_generations = 6, # Decrease if out of memory\n",
    "    max_prompt_length = 256,\n",
    "    max_completion_length = 200,\n",
    "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    max_steps = 5,\n",
    "    save_steps = 250,\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir = \"outputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9Mv8UZO5hz-"
   },
   "source": [
    "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
    "\n",
    "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
    "\n",
    "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
    "|------|---------------|-----------|------------|-------------------|----------|\n",
    "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
    "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
    "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vzOuSVCL_GA9"
   },
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        response_shape_reward_func,\n",
    "        soft_response_shape_reward_func,\n",
    "        valid_action_reward_func,\n",
    "        characteristics_complements_reward_func,\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlaUdxC_VHpz"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Now let's try the model we just trained! First, let's first try the model without any GRPO trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qtcz_lpbVC92"
   },
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"user\", \"content\" : \"Calculate pi.\"},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    [text],\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = None,\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Colxz9TAVMsi"
   },
   "source": [
    "And now with the LoRA we just trained with GRPO - we first save the LoRA first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AL-BcuB1VLIv"
   },
   "outputs": [],
   "source": [
    "model.save_lora(\"grpo_saved_lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwpbwnDBVRLg"
   },
   "source": [
    "Now we load the LoRA and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zf_OY5WMVOxF"
   },
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
    "    {\"role\" : \"user\", \"content\" : \"Calculate pi.\"},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    text,\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aDgFfhFYIAS"
   },
   "source": [
    "Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NUEmHFSYNTp"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NjXGTkp7YNtB"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52WMb3k_YPt8"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
    "\n",
    "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QyEjW-WuYQIm"
   },
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if True: model.push_to_hub_gguf(\"diegohc/finetuning-tests\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DOKTDA0F4to"
   },
   "source": [
    "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)\n",
    "\n",
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "Some other links:\n",
    "1. Llama 3.2 Conversational notebook. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)\n",
    "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
    "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
    "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
    "\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
    "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
    "\n",
    "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
