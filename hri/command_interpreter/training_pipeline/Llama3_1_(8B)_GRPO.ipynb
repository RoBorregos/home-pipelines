{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScY9EDIpusHv"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoBorregos/home-pipelines/blob/llm-train/Llama3_1_(8B)_GRPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us8KpUJWusHw"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwJRhodQusHx"
      },
      "source": [
        "# Set up drive directory for checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zCKrwpmTusHx",
        "outputId": "10e178fb-0a78-4ee3-ea02-668a12377286",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "checkpoint_dir = \"/content/drive/MyDrive/RoBorregos/model_checkpoints/grpo\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "os.listdir(checkpoint_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C56DB0SbusHx"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sqtv6lPusHy"
      },
      "source": [
        "**Read our [blog post](https://unsloth.ai/blog/r1-reasoning) for guidance on how to train reasoning models.**\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_lhalj_usHy"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HxWwjIbbusHy"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Skip restarting message in Colab\n",
        "import sys; modules = list(sys.modules.keys())\n",
        "for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "\n",
        "!pip install unsloth==2025.3.18 unsloth_zoo==2025.3.16\n",
        "!pip install vllm\n",
        "!pip install pillow==11.1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB1roCTEusHy"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHOLhQ-iusHy"
      },
      "source": [
        "Use `PatchFastRL` before all functions to patch GRPO and other RL algorithms!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wnDtMwnbusHz",
        "outputId": "552c7341-eec3-4cbf-d2e0-6b032e3518cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "INFO 04-15 19:40:19 [__init__.py:239] Automatically detected platform cuda.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel, PatchFastRL\n",
        "PatchFastRL(\"GRPO\", FastLanguageModel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zCkOq3gusHz"
      },
      "source": [
        "Load up `Llama 3.1 8B Instruct`, and set parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wRZVO9zCusHz",
        "outputId": "8564dafa-eeae-4355-b0b5-60ccc7af2b67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 757,
          "referenced_widgets": [
            "0db82db68e98427fbd72972466ff9cb0",
            "c624c36326e446c9ab51636e7cdd367e",
            "f03795c7b29b4acebee3a5dfa10203fb",
            "2a6cd2dec0414932b6a96bda6308addf",
            "dbdf6355223b4a9e9d8fb337e3ed21d9",
            "5ca74c78e2fe4f5b88edd50d9bfd6a7d",
            "85bf2cbe3ff641c9807b77a58b8f8421",
            "0c05aaae75454cbfb5e3648184279c6a",
            "6759f8d9e80047648239d68ff0ac7e86",
            "f035675430bc4d14baf7aaa4f2538f48",
            "88106b043adc458fae62eeac1297024b",
            "be9b3764974a44a6b601c072bb29fb4b",
            "6e33850e2b8641658cb733a25a014e8d",
            "18842838577b4a478541c4a5b341382e",
            "afba8e2a79914f7bbfab4fedaee2d66a",
            "cbfed2dc5d524033a4c0d3bdfc3f139b",
            "b285c20d1aed4ddca49a663c443f8dca",
            "da9db8ebc10341f1a8b6e8130bc858b5",
            "c8ee07eded474da895259026b6aaeaab",
            "df95a5f70c6a47999a2f2cfe4545a344",
            "5800b183c7554b3b98b0c3de173054a7",
            "8f7e42a3c34a4a5498d099f623014797",
            "2715e16f4ac44df9b890ad26c3c1187c",
            "dbeae62d2b8948e1b70e2a5a9cd78b79",
            "12f9621bb9e24d0297567a2fbf5e6997",
            "452eebfa3d31418db25ba889ed6b692d",
            "32b6e0b6c4424ef299416d9b6ca28b75",
            "58f7337960584621855d4f511a51a67f",
            "fa3b1d1fa89044a3aa9fcadcfa6e6ac0",
            "aae1d02679ef4f61836077d1d88d547d",
            "0bf68822ad8b4f01a9170cf6a02ec2ad",
            "62b21353a533439b8e01873c80e1d1c9",
            "71f7c930023348dc84d1cb63069b0f72"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.3.18: Fast Llama patching. Transformers: 4.51.1. vLLM: 0.8.4.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: vLLM loading unsloth/deepseek-r1-distill-llama-8b-unsloth-bnb-4bit with actual GPU utilization = 59.43%\n",
            "Unsloth: Your GPU has CUDA compute capability 7.5 with VRAM = 14.74 GB.\n",
            "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 512. Num Sequences = 160.\n",
            "Unsloth: vLLM's KV Cache can use up to 2.59 GB. Also swap space = 2 GB.\n",
            "WARNING 04-15 19:43:20 [config.py:2836] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 04-15 19:43:35 [config.py:689] This model supports multiple tasks: {'classify', 'score', 'generate', 'embed', 'reward'}. Defaulting to 'generate'.\n",
            "WARNING 04-15 19:43:35 [arg_utils.py:1731] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
            "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.1.mlp'], 'llm_int8_threshold': 6.0}\n",
            "INFO 04-15 19:43:35 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='unsloth/deepseek-r1-distill-llama-8b-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/deepseek-r1-distill-llama-8b-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=512, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/deepseek-r1-distill-llama-8b-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":160}, use_cached_outputs=False, \n",
            "INFO 04-15 19:43:38 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "INFO 04-15 19:43:38 [cuda.py:289] Using XFormers backend.\n",
            "INFO 04-15 19:43:39 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
            "INFO 04-15 19:43:39 [model_runner.py:1110] Starting to load model unsloth/deepseek-r1-distill-llama-8b-unsloth-bnb-4bit...\n",
            "INFO 04-15 19:43:39 [loader.py:1166] Loading weights with BitsAndBytes quantization. May take a while ...\n",
            "INFO 04-15 19:43:40 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
            "INFO 04-15 19:44:49 [weight_utils.py:281] Time spent downloading weights for unsloth/deepseek-r1-distill-llama-8b-unsloth-bnb-4bit: 68.975931 seconds\n",
            "INFO 04-15 19:44:49 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0db82db68e98427fbd72972466ff9cb0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be9b3764974a44a6b601c072bb29fb4b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 04-15 19:45:46 [punica_selector.py:18] Using PunicaWrapperGPU.\n",
            "INFO 04-15 19:45:46 [model_runner.py:1146] Model loading took 5.7737 GiB and 126.704938 seconds\n",
            "INFO 04-15 19:45:58 [worker.py:267] Memory profiling takes 11.76 seconds\n",
            "INFO 04-15 19:45:58 [worker.py:267] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.59) = 8.76GiB\n",
            "INFO 04-15 19:45:58 [worker.py:267] model weights take 5.77GiB; non_torch_memory takes 0.03GiB; PyTorch activation peak memory takes 0.74GiB; the rest of the memory reserved for KV Cache is 2.22GiB.\n",
            "INFO 04-15 19:45:59 [executor_base.py:112] # cuda blocks: 1136, # CPU blocks: 1024\n",
            "INFO 04-15 19:45:59 [executor_base.py:117] Maximum concurrency for 512 tokens per request: 35.50x\n",
            "INFO 04-15 19:46:00 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Capturing CUDA graph shapes:   0%|          | 0/23 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2715e16f4ac44df9b890ad26c3c1187c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 04-15 19:47:08 [model_runner.py:1598] Graph capturing finished in 67 secs, took 0.53 GiB\n",
            "INFO 04-15 19:47:08 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 81.37 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.3.18 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import is_bfloat16_supported\n",
        "import torch\n",
        "max_seq_length = 512 # Can increase for longer reasoning traces\n",
        "lora_rank = 32 # Larger rank = smarter, but slower\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True, # False for LoRA 16bit\n",
        "    fast_inference = True, # Enable vLLM fast inference\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.6, # Reduce if out of memory\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ], # Remove QKVO if out of memory\n",
        "    lora_alpha = lora_rank,\n",
        "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdRqcz7qusHz"
      },
      "source": [
        "### Data Prep\n",
        "<a name=\"Data\"></a>\n",
        "\n",
        "We directly leverage [@willccbb](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb) for data prep and all reward functions. You are free to create your own!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "2ciqu6lGusHz"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from datasets import load_dataset, Dataset\n",
        "from typing import List, Optional\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Load and prep dataset\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format:\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "XML_COT_FORMAT = \"\"\"\\\n",
        "<reasoning>\n",
        "{reasoning}\n",
        "</reasoning>\n",
        "<answer>\n",
        "{answer}\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    answer = text.split(\"<answer>\")[-1]\n",
        "    answer = answer.split(\"</answer>\")[0]\n",
        "    return answer.strip()\n",
        "\n",
        "def extract_hash_answer(text: str) -> str | None:\n",
        "    if \"####\" not in text:\n",
        "        return None\n",
        "    return text.split(\"####\")[1].strip()\n",
        "\n",
        "# uncomment middle messages for 1-shot prompting\n",
        "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
        "    data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore\n",
        "    data = data.map(lambda x: { # type: ignore\n",
        "        'prompt': [\n",
        "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "            {'role': 'user', 'content': x['question']}\n",
        "        ],\n",
        "        'answer': extract_hash_answer(x['answer'])\n",
        "    }) # type: ignore\n",
        "    return data # type: ignore\n",
        "\n",
        "import json\n",
        "\n",
        "def load_custom_dataset(file_path):\n",
        "    \"\"\"\n",
        "    Load custom dataset from JSON file and format for GRPO training\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        # Each line is a separate JSON object\n",
        "        data = [json.loads(line) for line in f if line.strip()]\n",
        "\n",
        "    # Create dataset dictionary with the format expected by GRPO trainer\n",
        "    dataset_dict = {\n",
        "        'prompt': [],\n",
        "        'answer': []\n",
        "    }\n",
        "\n",
        "    for item in data:\n",
        "        messages = item['messages']\n",
        "        # The prompt is all messages except the last one (assistant's response)\n",
        "        dataset_dict['prompt'].append(messages[:-1])\n",
        "        # The answer is the content of the assistant's message\n",
        "        dataset_dict['answer'].append(messages[-1]['content'])\n",
        "\n",
        "    # Convert to HuggingFace dataset\n",
        "    dataset = Dataset.from_dict(dataset_dict)\n",
        "    return dataset\n",
        "\n",
        "def load_gpsr_dataset(file_path):\n",
        "    \"\"\"\n",
        "    Load custom dataset from JSON file and format for GRPO training\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Create dataset dictionary with the format expected by GRPO trainer\n",
        "    dataset_dict = {\n",
        "        'question': [],\n",
        "        'prompt': [],\n",
        "        'answer': []\n",
        "    }\n",
        "\n",
        "    for entry in data:\n",
        "        dataset_dict['question'].append(entry['string_cmd'])\n",
        "        dataset_dict['prompt'].append([\n",
        "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "            {'role': 'user', 'content': entry['string_cmd']}\n",
        "        ])\n",
        "        dataset_dict['answer'].append(entry['structured_cmd'])\n",
        "\n",
        "    # Convert to HuggingFace dataset\n",
        "    dataset = Dataset.from_dict(dataset_dict)\n",
        "    return dataset\n",
        "\n",
        "# dataset = get_gsm8k_questions()\n",
        "\n",
        "# print(dataset)\n",
        "# dataset = load_custom_dataset()\n",
        "dataset = load_gpsr_dataset('dataset.json')\n",
        "\n",
        "# Define expected response structure\n",
        "class CommandShape(BaseModel):\n",
        "    action: str = Field(description=\"The action to be performed\")\n",
        "    characteristic: Optional[str] = Field(description=\"A characteristic related to the action\")\n",
        "    complement: Optional[str] = Field(description=\"A complement related to the action\")\n",
        "\n",
        "class CommandListShape(BaseModel):\n",
        "    commands: List[CommandShape]\n",
        "\n",
        "available_actions = [\n",
        "    \"go\",\"find_object\",\"pick\",\"find_person\",\"find_person_by_name\",\"count\",\n",
        "    \"contextual_say\",\"find_person_info\",\"visual_info\",\"ask_answer_question\",\n",
        "    \"follow_person_until\",\"guide_to\",\"give\",\"say\",\"place\"\n",
        "]\n",
        "\n",
        "def verify_response_shape(response: str) -> bool:\n",
        "    \"\"\"Check if response is a valid CommandListShape.\"\"\"\n",
        "    try:\n",
        "        parsed = CommandListShape.model_validate_json(response)\n",
        "        return isinstance(parsed, CommandListShape)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def verify_soft_response_shape(response: str) -> bool:\n",
        "    \"\"\"Check if response contains something resembling commands.\"\"\"\n",
        "    return \"commands\" in response and \"action\" in response\n",
        "\n",
        "def has_valid_actions(response: str) -> bool:\n",
        "    \"\"\"Check if the response contains only available actions.\"\"\"\n",
        "    try:\n",
        "        parsed = CommandListShape.model_validate_json(response)\n",
        "        for cmd in parsed.commands:\n",
        "            if cmd.action not in available_actions:\n",
        "                return False\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def has_characteristics_and_complements(response: str) -> bool:\n",
        "    \"\"\"Check if complement is present but characteristic is not\"\"\"\n",
        "    try:\n",
        "        parsed = CommandListShape.model_validate_json(response)\n",
        "        for cmd in parsed.commands:\n",
        "            if cmd.complement is not None and cmd.characteristic is None:\n",
        "                return False\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "# Combined reward functions\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    print(completions)\n",
        "    print(completions[0])\n",
        "    print(completions[0][0])\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    extracted_contents = [extract_xml_answer(c) for c in contents]\n",
        "\n",
        "    print(f\"Completion: {extracted_contents[0]}, Answer: {answer}\")\n",
        "\n",
        "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_contents, answer)]\n",
        "\n",
        "def response_shape_reward_func(completions, **kwargs) -> list[float]:\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    extracted_contents = [extract_xml_answer(c) for c in contents]\n",
        "\n",
        "    return [1.0 if verify_response_shape(c) else 0.0 for c in extracted_contents]\n",
        "\n",
        "def soft_response_shape_reward_func(completions, **kwargs) -> list[float]:\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    extracted_contents = [extract_xml_answer(c) for c in contents]\n",
        "\n",
        "    return [0.5 if verify_soft_response_shape(c) else 0.0 for c in extracted_contents]\n",
        "\n",
        "def valid_action_reward_func(completions, **kwargs) -> list[float]:\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    extracted_contents = [extract_xml_answer(c) for c in contents]\n",
        "\n",
        "    return [1.0 if has_valid_actions(c) else 0.0 for c in extracted_contents]\n",
        "\n",
        "def characteristics_complements_reward_func(completions, **kwargs) -> list[float]:\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    extracted_contents = [extract_xml_answer(c) for c in contents]\n",
        "\n",
        "    return [1.0 if has_characteristics_and_complements(c) else 0.0 for c in extracted_contents]\n",
        "\n",
        "def strict_xml_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "def soft_xml_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "def count_xml(text) -> float:\n",
        "    count = 0.0\n",
        "    if text.count(\"<reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n<answer>\\n\") == 1:\n",
        "        count += 0.125\n",
        "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
        "    if text.count(\"\\n</answer>\") == 1:\n",
        "        count += 0.125\n",
        "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
        "    return count\n",
        "\n",
        "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    return [count_xml(c) for c in contents]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST_Ey-YGusH0"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "\n",
        "Now set up GRPO Trainer and all configurations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RGW62lu_usH0",
        "outputId": "e0df7637-4493-4750-f318-b8ad819131b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
            "We will change the batch size of 1 to the `num_generations` of 6\n"
          ]
        }
      ],
      "source": [
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    use_vllm = True, # use vLLM for fast inference!\n",
        "    learning_rate = 5e-6,\n",
        "    adam_beta1 = 0.9,\n",
        "    adam_beta2 = 0.99,\n",
        "    weight_decay = 0.1,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    optim = \"paged_adamw_8bit\",\n",
        "    logging_steps = 1,\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
        "    num_generations = 6, # Decrease if out of memory\n",
        "    max_prompt_length = 256,\n",
        "    max_completion_length = 200,\n",
        "    num_train_epochs = 1, # Set to 1 for a full training run\n",
        "    # max_steps = 5,\n",
        "    max_grad_norm = 0.1,\n",
        "    report_to = \"none\", # Can use Weights & Biases\n",
        "    output_dir = checkpoint_dir,\n",
        "    save_strategy = \"steps\",\n",
        "    save_steps = 50,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlMZLEuausH0"
      },
      "source": [
        "[link text](https://)And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
        "\n",
        "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
        "\n",
        "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
        "|------|---------------|-----------|------------|-------------------|----------|\n",
        "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
        "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
        "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSoLBdWiusH0",
        "outputId": "94c3b770-efb1-4c5b-9235-3734d3ba8a42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No valid checkpoint found in output directory (/content/drive/MyDrive/RoBorregos/model_checkpoints/grpo)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 460 | Num Epochs = 1 | Total steps = 460\n",
            "O^O/ \\_/ \\    Batch size per device = 6 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (6 x 1 x 1) = 6\n",
            " \"-____-\"     Trainable parameters = 83,886,080/8,000,000,000 (1.05% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[{'role': 'assistant', 'content': 'Alright, so I\\'m trying to figure out how to answer the quiz from the person sitting to my left in the office. Hmm, first I should probably understand what the quiz is about. The message didn\\'t specify, so I might need to make some assumptions. Since the person pointed me to respond, maybe they provided a specific question earlier that I need to address.\\n\\nWait, in the initial message, it just says \"answer the quiz of the person pointing to the left in the office.\" I\\'m a bit confused. Maybe they referred to a previous conversation or a shared document where the quiz was mentioned. Since I don\\'t have access to that, perhaps I should prompt them to provide more details.\\n\\nI should consider if the quiz is about general knowledge, perhaps a puzzle, or something related to our work. Maybe it\\'s about office procedures, team trivia, or something fun. I don\\'t want to assume too much and give the wrong answer. It\\'s better to ask for clarification to ensure'}], [{'role': 'assistant', 'content': \"Okay, so I'm trying to figure out the answer to the quiz that the person to my left has. The quiz is about animals, specifically regarding what their body parts look like. I don't know the question exactly, but I'll try to think through it as if I did. \\n\\nHmm, let's say the question is about a specific animal's trait or feature. Maybe it's about the tail? Or maybe the legs. Wait, another common feature could be the number of legs or whether they're hoofed or clawed. Alternatively, it could be about the animals' habitat or diet.\\n\\nI should consider some basic animal facts. If I think of wild animals, perhaps like a lion versus a zebra, or a horse versus a deer. Maybe the question is about whether a certain animal has four legs or two. Or maybe it's about the number of legs when they're young versus adult. Or maybe it's about their diet‚Äîlike if they're herbivores or\"}], [{'role': 'assistant', 'content': \"Alright, so I need to figure out the answer to a quiz that someone else is working on. The quiz is about, well, I'm not exactly sure what it's about since the user didn't specify. But the user mentioned pointing to the left in the office, which might be a directional clue or a reference to a specific problem or question they had earlier. \\n\\nHmm, maybe they had a diagram or a map involved where something is to the left. Without more details, it's a bit tricky, but I'll try to think of common types of questions that might come up in a quiz that involve directions. Perhaps it's about identifying objects, locations, or maybe a sequence of events. \\n\\nIf the quiz is about geography, maybe they're pointing out a country or a city on a map. Or it could be about patterns or shapes. Another possibility is it's related to language arts, like identifying a character's position in a story. Or perhaps it's a math problem where\"}], [{'role': 'assistant', 'content': \"Alright, so I'm trying to figure out how to answer the quiz that the person to my left is pointing at. But wait, I don't have any context about the quiz. Maybe they just wanted me to describe my reasoning process, not the actual answer. Or perhaps the initial message is incomplete. I need to make sure I understand what exactly is being asked. If the quiz is about something specific, like math or history, I should think through the steps to solve it. But without knowing the subject, it's hard to proceed. Maybe I should ask for more details or clarify what the quiz is about. Once I have that information, I can provide a step-by-step explanation or the correct answer.\\n</think>\\n\\n<reasoning>\\nThe quiz mentioned by the user refers to a query about answering a quiz that someone else pointed out. Since the quiz's content isn't specified, it's impossible to provide a direct answer without knowing the subject. To assist effectively, I should ask for more\"}], [{'role': 'assistant', 'content': \"Okay, so I'm trying to figure out how to answer the quiz question from the person sitting to my left. Let's see, I just got here, so I'm a bit new. But I'll try to use what I know.\\n\\nFirst, I need to understand what the quiz is about. The user mentioned something about the office seating arrangement. Hmm, maybe it's about the structure or the people who work there. I should think about the dynamics of the office. Maybe it's a team setup or something related to management.\\n\\nWait, they mentioned pointing to the left. So perhaps it's about the person sitting to their left. Maybe it's a question about their role or responsibilities? Or maybe it's a question about the team they're on. Let me try to break it down.\\n\\nI should consider different aspects. Maybe it's about their position relative to the door or their desk setup. Or perhaps it's about their colleagues they interact with. I'm a bit unsure. I\"}], [{'role': 'assistant', 'content': \"Okay, so I have this question to answer about the quiz I need to take based on the characters from the left side of the office. I'm a bit new to this, so I should probably break it down step by step. Let me try to figure this out.\\n\\nFirst, I need to remember who the characters are on the left side of the office. I think the left side is where the main characters are. There's Dwight, Jim, Pam, Michael, and probably some others like Kevin and Oscar. Maybe others like Toby or Darryl as well?\\n\\nWait, the person in the question is pointing to the left, so they're probably pointing towards the main group. The quiz is about something they're going over or maybe a question related to them. I should think about what a common quiz question might be about.\\n\\nIf I were to take a quiz about them, it could cover their personalities, funny moments, or maybe something related to the show's structure, like the branches\"}]]\n",
            "[{'role': 'assistant', 'content': 'Alright, so I\\'m trying to figure out how to answer the quiz from the person sitting to my left in the office. Hmm, first I should probably understand what the quiz is about. The message didn\\'t specify, so I might need to make some assumptions. Since the person pointed me to respond, maybe they provided a specific question earlier that I need to address.\\n\\nWait, in the initial message, it just says \"answer the quiz of the person pointing to the left in the office.\" I\\'m a bit confused. Maybe they referred to a previous conversation or a shared document where the quiz was mentioned. Since I don\\'t have access to that, perhaps I should prompt them to provide more details.\\n\\nI should consider if the quiz is about general knowledge, perhaps a puzzle, or something related to our work. Maybe it\\'s about office procedures, team trivia, or something fun. I don\\'t want to assume too much and give the wrong answer. It\\'s better to ask for clarification to ensure'}]\n",
            "{'role': 'assistant', 'content': 'Alright, so I\\'m trying to figure out how to answer the quiz from the person sitting to my left in the office. Hmm, first I should probably understand what the quiz is about. The message didn\\'t specify, so I might need to make some assumptions. Since the person pointed me to respond, maybe they provided a specific question earlier that I need to address.\\n\\nWait, in the initial message, it just says \"answer the quiz of the person pointing to the left in the office.\" I\\'m a bit confused. Maybe they referred to a previous conversation or a shared document where the quiz was mentioned. Since I don\\'t have access to that, perhaps I should prompt them to provide more details.\\n\\nI should consider if the quiz is about general knowledge, perhaps a puzzle, or something related to our work. Maybe it\\'s about office procedures, team trivia, or something fun. I don\\'t want to assume too much and give the wrong answer. It\\'s better to ask for clarification to ensure'}\n",
            "Completion: Alright, so I'm trying to figure out how to answer the quiz from the person sitting to my left in the office. Hmm, first I should probably understand what the quiz is about. The message didn't specify, so I might need to make some assumptions. Since the person pointed me to respond, maybe they provided a specific question earlier that I need to address.\n",
            "\n",
            "Wait, in the initial message, it just says \"answer the quiz of the person pointing to the left in the office.\" I'm a bit confused. Maybe they referred to a previous conversation or a shared document where the quiz was mentioned. Since I don't have access to that, perhaps I should prompt them to provide more details.\n",
            "\n",
            "I should consider if the quiz is about general knowledge, perhaps a puzzle, or something related to our work. Maybe it's about office procedures, team trivia, or something fun. I don't want to assume too much and give the wrong answer. It's better to ask for clarification to ensure, Answer: ['{\"commands\":[{\"action\":\"go\",\"complement\":\"office\",\"characteristic\":\"\"},{\"action\":\"find_person\",\"complement\":\"person pointing to the left\",\"characteristic\":\"\"},{\"action\":\"ask_answer_question\",\"complement\":\"\",\"characteristic\":\"\"}]}', '{\"commands\":[{\"action\":\"go\",\"complement\":\"office\",\"characteristic\":\"\"},{\"action\":\"find_person\",\"complement\":\"person pointing to the left\",\"characteristic\":\"\"},{\"action\":\"ask_answer_question\",\"complement\":\"\",\"characteristic\":\"\"}]}', '{\"commands\":[{\"action\":\"go\",\"complement\":\"office\",\"characteristic\":\"\"},{\"action\":\"find_person\",\"complement\":\"person pointing to the left\",\"characteristic\":\"\"},{\"action\":\"ask_answer_question\",\"complement\":\"\",\"characteristic\":\"\"}]}', '{\"commands\":[{\"action\":\"go\",\"complement\":\"office\",\"characteristic\":\"\"},{\"action\":\"find_person\",\"complement\":\"person pointing to the left\",\"characteristic\":\"\"},{\"action\":\"ask_answer_question\",\"complement\":\"\",\"characteristic\":\"\"}]}', '{\"commands\":[{\"action\":\"go\",\"complement\":\"office\",\"characteristic\":\"\"},{\"action\":\"find_person\",\"complement\":\"person pointing to the left\",\"characteristic\":\"\"},{\"action\":\"ask_answer_question\",\"complement\":\"\",\"characteristic\":\"\"}]}', '{\"commands\":[{\"action\":\"go\",\"complement\":\"office\",\"characteristic\":\"\"},{\"action\":\"find_person\",\"complement\":\"person pointing to the left\",\"characteristic\":\"\"},{\"action\":\"ask_answer_question\",\"complement\":\"\",\"characteristic\":\"\"}]}']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  4/460 01:38 < 6:15:40, 0.02 it/s, Epoch 0.01/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>reward</th>\n",
              "      <th>reward_std</th>\n",
              "      <th>completion_length</th>\n",
              "      <th>kl</th>\n",
              "      <th>rewards / correctness_reward_func</th>\n",
              "      <th>rewards / response_shape_reward_func</th>\n",
              "      <th>rewards / soft_response_shape_reward_func</th>\n",
              "      <th>rewards / valid_action_reward_func</th>\n",
              "      <th>rewards / characteristics_complements_reward_func</th>\n",
              "      <th>rewards / strict_xml_format_reward_func</th>\n",
              "      <th>rewards / soft_xml_format_reward_func</th>\n",
              "      <th>rewards / xmlcount_reward_func</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.020833</td>\n",
              "      <td>0.051031</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.020833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[{'role': 'assistant', 'content': \"Alright, so I need to figure out how to help the user with their request. They want me to find a snack in the office and bring it to the person sitting in the living room. Hmm, okay, let me break this down.\\n\\nFirst, I should think about the steps involved. Find a snack in the office. That means I need to know where the snacks are located. Usually, snacks are kept in a kitchen or a pantry. I should check those areas. Maybe the user can tell me where the snacks are stored. Oh, wait, the user didn't specify that they have a specific type of snack in mind. So I might need to ask them about their preference to choose the right snack.\\n\\nNext, once I find the snack, I need to fetch it. That's straightforward; I just need to go over to the office and pick it up. But wait, am I allowed to go into the kitchen or office? I should confirm whether the area where the snacks are\"}], [{'role': 'assistant', 'content': 'Okay, let me try to figure out how to solve this. The user wants me to find a snack in the office and then bring it to the sitting person in the living room. Hmm, where do I start?\\n\\nFirst, I need to understand the task clearly. They said \"find a snack in the office.\" So, I need to locate some food somewhere in the office. But what kind of snack? They didn\\'t specify, so I guess any snack would do, like crisps or a chocolate bar. Maybe I should pick something that\\'s easily accessible to make it quick.\\n\\nNext, I need to fetch the snack. So, I\\'ll have to move around the office, probably near the kitchen area where snacks are usually kept. Maybe the snack shelf is near the break room or the kitchenette. I should check the usual places where snacks are stored.\\n\\nOnce I find the snack, I need to bring it to the living room. So, I\\'ll have to go from the office'}], [{'role': 'assistant', 'content': \"Okay, so the user wants me to find a snack in the office and bring it to the person sitting in the living room. Let me try to figure out the best way to approach this.\\n\\nFirst, I should consider the possible places where snacks are kept. Probably, there's a snack drawer or a bench where the person usually keeps their snacks. I need to check each likely spot step by step. Maybe the person uses a specific container or has a favorite type of snack. Also, I should consider if the snack is something the person usually eats, like chips, fruits, or maybe some/orderly eaten.\\n\\nNext, once I find the snack, I need to make sure I bring it to the right person in the living room. I should check if the person is indeed in the living room and not somewhere else. If they are there, I can approach them politely and offer the snack. It's important to be friendly and clear about what snack I found, so the person knows exactly which\"}], [{'role': 'assistant', 'content': 'Alright, I need to figure out how to respond to the user\\'s request. The user wrote, \"find a snack in the office then fetch it and bring it to the sitting person in the living room.\" Hmm, okay, so the user is asking me to perform two actions: first, find a snack in the office, and then bring it to someone sitting in the living room.\\n\\nWait, I\\'m an AI, not a physical being, so I can\\'t physically move around the office or the house. So I need to make sure I let the user know that I can\\'t do that directly. But maybe I can suggest a way they can do it themselves or offer an alternative.\\n\\nI should also consider if there\\'s a specific snack they might need, or if they just want a general suggestion. Maybe I can ask them what they\\'re in the mood for. It\\'s important to be helpful without overstepping my capabilities.\\n\\nSo, the best approach is probably to acknowledge that I can'}], [{'role': 'assistant', 'content': \"Okay, so I need to figure out how to solve the problem of fetching a snack from the office and bringing it to the person sitting in the living room. Alright, let me start by understanding the problem properly.\\n\\nFirst, I should consider the scenario: there's someone in the living room, probably feeling hungry, and they want a snack. So the main goal is to get them a snack from the office and deliver it to them. But how do I approach this?\\n\\nI think the first step is to identify where the snacks are kept in the office. Usually, snacks are stored in a pantry or a kitchen area. Maybe the office has a shared fridge or a cupboard where snacks are kept. I should check that first. Let me visualize the office layout: is the snack located on a shelf, in a box, or maybe in a specific container? Also, are there any specific snacks the person in the living room prefers? If I knew, I could be more efficient, but maybe I\"}], [{'role': 'assistant', 'content': 'Alright, so I just got a message that says, \"find a snack in the office then fetch it and bring it to the sitting person in the living room.\" Hmm, okay, let me try to figure out what the user is asking for here. \\n\\nFirst, I need to understand the context. It sounds like the user is in an office environment. They want me to find a snack in the office, then fetch it and bring it to someone who\\'s sitting in the living room. Wait, but am I an AI, so how do I navigate through the office? I don\\'t have a physical body, right? So maybe they\\'re talking about a scenario where someone has to do this, not me personally.\\n\\nWait, perhaps the user is role-playing or simulating a task. Maybe they\\'re in a game or a scenario where they need to perform this action. Or maybe it\\'s a test to see if I can generate instructions or a dialogue for such a task. \\n\\nI should'}]]\n",
            "[{'role': 'assistant', 'content': \"Alright, so I need to figure out how to help the user with their request. They want me to find a snack in the office and bring it to the person sitting in the living room. Hmm, okay, let me break this down.\\n\\nFirst, I should think about the steps involved. Find a snack in the office. That means I need to know where the snacks are located. Usually, snacks are kept in a kitchen or a pantry. I should check those areas. Maybe the user can tell me where the snacks are stored. Oh, wait, the user didn't specify that they have a specific type of snack in mind. So I might need to ask them about their preference to choose the right snack.\\n\\nNext, once I find the snack, I need to fetch it. That's straightforward; I just need to go over to the office and pick it up. But wait, am I allowed to go into the kitchen or office? I should confirm whether the area where the snacks are\"}]\n",
            "{'role': 'assistant', 'content': \"Alright, so I need to figure out how to help the user with their request. They want me to find a snack in the office and bring it to the person sitting in the living room. Hmm, okay, let me break this down.\\n\\nFirst, I should think about the steps involved. Find a snack in the office. That means I need to know where the snacks are located. Usually, snacks are kept in a kitchen or a pantry. I should check those areas. Maybe the user can tell me where the snacks are stored. Oh, wait, the user didn't specify that they have a specific type of snack in mind. So I might need to ask them about their preference to choose the right snack.\\n\\nNext, once I find the snack, I need to fetch it. That's straightforward; I just need to go over to the office and pick it up. But wait, am I allowed to go into the kitchen or office? I should confirm whether the area where the snacks are\"}\n",
            "Completion: Alright, so I need to figure out how to help the user with their request. They want me to find a snack in the office and bring it to the person sitting in the living room. Hmm, okay, let me break this down.\n",
            "\n",
            "First, I should think about the steps involved. Find a snack in the office. That means I need to know where the snacks are located. Usually, snacks are kept in a kitchen or a pantry. I should check those areas. Maybe the user can tell me where the snacks are stored. Oh, wait, the user didn't specify that they have a specific type of snack in mind. So I might need to ask them about their preference to choose the right snack.\n",
            "\n",
            "Next, once I find the snack, I need to fetch it. That's straightforward; I just need to go over to the office and pick it up. But wait, am I allowed to go into the kitchen or office? I should confirm whether the area where the snacks are, Answer: ['{\"commands\":[{\"action\":\"go\",\"complement\":\"office\",\"characteristic\":\"\"},{\"action\":\"find_object\",\"complement\":\"office\",\"characteristic\":\"snack\"},{\"action\":\"pick\",\"complement\":\"snack\",\"characteristic\":\"\"},{\"action\":\"go\",\"complement\":\"living room\",\"characteristic\":\"\"},{\"action\":\"find_person\",\"complement\":\"sitting person\",\"characteristic\":\"\"},{\"action\":\"give\",\"complement\":\"\",\"characteristic\":\"\"}]}', '{\"commands\":[{\"action\":\"go\",\"complement\":\"office\",\"characteristic\":\"\"},{\"action\":\"find_object\",\"complement\":\"office\",\"characteristic\":\"snack\"},{\"action\":\"pick\",\"complement\":\"snack\",\"characteristic\":\"\"},{\"action\":\"go\",\"complement\":\"living room\",\"characteristic\":\"\"},{\"action\":\"find_person\",\"complement\":\"sitting person\",\"characteristic\":\"\"},{\"action\":\"give\",\"complement\":\"\",\"characteristic\":\"\"}]}', '{\"commands\":[{\"action\":\"go\",\"complement\":\"office\",\"characteristic\":\"\"},{\"action\":\"find_object\",\"complement\":\"office\",\"characteristic\":\"snack\"},{\"action\":\"pick\",\"complement\":\"snack\",\"characteristic\":\"\"},{\"action\":\"go\",\"complement\":\"living room\",\"characteristic\":\"\"},{\"action\":\"find_person\",\"complement\":\"sitting person\",\"characteristic\":\"\"},{\"action\":\"give\",\"complement\":\"\",\"characteristic\":\"\"}]}', '{\"commands\":[{\"action\":\"go\",\"complement\":\"office\",\"characteristic\":\"\"},{\"action\":\"find_object\",\"complement\":\"office\",\"characteristic\":\"snack\"},{\"action\":\"pick\",\"complement\":\"snack\",\"characteristic\":\"\"},{\"action\":\"go\",\"complement\":\"living room\",\"characteristic\":\"\"},{\"action\":\"find_person\",\"complement\":\"sitting person\",\"characteristic\":\"\"},{\"action\":\"give\",\"complement\":\"\",\"characteristic\":\"\"}]}', '{\"commands\":[{\"action\":\"go\",\"complement\":\"office\",\"characteristic\":\"\"},{\"action\":\"find_object\",\"complement\":\"office\",\"characteristic\":\"snack\"},{\"action\":\"pick\",\"complement\":\"snack\",\"characteristic\":\"\"},{\"action\":\"go\",\"complement\":\"living room\",\"characteristic\":\"\"},{\"action\":\"find_person\",\"complement\":\"sitting person\",\"characteristic\":\"\"},{\"action\":\"give\",\"complement\":\"\",\"characteristic\":\"\"}]}', '{\"commands\":[{\"action\":\"go\",\"complement\":\"office\",\"characteristic\":\"\"},{\"action\":\"find_object\",\"complement\":\"office\",\"characteristic\":\"snack\"},{\"action\":\"pick\",\"complement\":\"snack\",\"characteristic\":\"\"},{\"action\":\"go\",\"complement\":\"living room\",\"characteristic\":\"\"},{\"action\":\"find_person\",\"complement\":\"sitting person\",\"characteristic\":\"\"},{\"action\":\"give\",\"complement\":\"\",\"characteristic\":\"\"}]}']\n",
            "Unsloth: Will smartly offload gradients to save VRAM!\n",
            "[[{'role': 'assistant', 'content': 'Okay, so I need to figure out who\\'s at the sink and tell that name to the person at the TV stand. Hmm, the user gave a response that says the person at the sink is \"The Sinking Guy\" and the person at the TV is \"The TV Guy.\" But wait, how does that make sense? If I\\'m supposed to tell the name at the sink to the person at the TV, shouldn\\'t the information flow from sink to TV?\\n\\nFirst, I should visualize the setup. Maybe they\\'re in a kitchen or a living room. The sink is probably near the kitchen, and the TV is in the living area. So, Person A is at the sink, and Person B is at the TV. \\n\\nWhy does the first response say \"The Sinking Guy\" and \"The TV Guy\"? Maybe it\\'s a play on words or nicknames. \"Sinking Guy\" could imply that he\\'s always at the sink, like his name is \"Sinking'}], [{'role': 'assistant', 'content': 'Okay, so I\\'m trying to figure out the name of the person at the sink. Hmm, where do I start? Let me think about what the user is asking. They mentioned the person at the sink and the person at the tv stand. Wait, no, they actually phrased it as \"tell the name of the person at the sink to the person at the tv stand.\" So, the user wants me to figure out the name of the person at the sink and then tell that name to the person at the tv stand.\\n\\nFirst, I need to understand the scenario. There are two people involved: one at the sink and one at the TV stand. The user is probably in a kitchen setting, maybe cooking or doing dishes, and they want to know the name of the person at the sink to communicate with them. But wait, without more context, it\\'s a bit vague.\\n\\nWait, the user didn\\'t provide a specific name or any clues about the situation. So,'}], [{'role': 'assistant', 'content': \"Okay, so I need to figure out the name of the person at the sink. The user mentioned that the person at the sink is the same as the person at the TV stand. Hmm, wait, that seems a bit confusing. Let me try to break it down.\\n\\nFirst, I should probably visualize the scenario. There are two people here: one is at the sink, and the other is at the TV stand. The user wants me to tell the name of the person at the sink to the person at the TV stand. Wait, but if they are the same person, then shouldn't the person at the TV stand already know their own name? Maybe there's a misunderstanding here.\\n\\nAlternatively, perhaps the user is implying that the person at the sink is someone else, and they want me to figure out their name based on some clues. But I don't have any additional information or context from the user besides this single statement. Let me think.\\n\\nWait, maybe there's a typo or\"}], [{'role': 'assistant', 'content': \"Okay, so I'm trying to figure out the name of the person at the sink. The user is asking for the name of the person at the sink, and I need to tell the person at the TV stand about it. Hmm, but wait, where exactly is the TV stand? Is this a specific room, like a kitchen, a living room, or maybe a bedroom? Without knowing the layout of the rooms, it's a bit tricky to determine which person is at the sink.\\n\\nLet me think differently. Maybe the sink is in the kitchen, so the person at the sink is probably the chef or someone preparing food. If it's the bathroom, though, the person at the sink would be someone washing their hands or brushing their teeth. But since the question is telling me to tell the person at the TV stand, maybe the rooms are adjacent. Like, in some houses, the kitchen is near the living room where the TV is.\\n\\nWait, maybe it's a question about healthcare\"}], [{'role': 'assistant', 'content': \"Alright, so I've got this query here. The user wants me to tell the name of the person at the sink to the person at the tv stand. Hmm, okay, first off, I need to understand the setup. There are two people involved: one at the sink and one at the TV stand. They want me to figure out the name of the person at the sink so that the person at the TV can know it.\\n\\nWait, but I'm not given any context. I mean, the query is pretty straightforward but missing some information. I can't just pull names out of thin air. Maybe the user is in a situation where there are two people, and they need to refer to each other by name. Let's see, perhaps they're in a home setting, like a kitchen and a living room with a TV. Like, someone is washing dishes and the other is watching TV. So the user is either one of them or maybe a third person trying to introduce them.\\n\\n\"}], [{'role': 'assistant', 'content': \"Alright, so I need to figure out who's at the sink. Let me try to visualize the scene they're describing. There's a kitchen, I think, with the person at the sink, and another person at the TV stand. The person at the sink is washing dishes, probably because that's a common activity near a sink. The person at the TV stand is more focused on the screen, maybe watching something or playing a game.\\n\\nHmm, they mentioned a baby on the floor. So, the baby is on the floor, maybe playing with a toy or crawling around. That makes sense. But how do I connect the baby to the sink? Maybe the person at the sink is the mother or father, taking care of the baby while doing dishes. Or perhaps the baby is nearby, like on a chair next to the sink.\\n\\nWait, the person at the TV stand could be another family member, maybe an older sibling or someone else visiting. The baby could be the youngest member of\"}]]\n",
            "[{'role': 'assistant', 'content': 'Okay, so I need to figure out who\\'s at the sink and tell that name to the person at the TV stand. Hmm, the user gave a response that says the person at the sink is \"The Sinking Guy\" and the person at the TV is \"The TV Guy.\" But wait, how does that make sense? If I\\'m supposed to tell the name at the sink to the person at the TV, shouldn\\'t the information flow from sink to TV?\\n\\nFirst, I should visualize the setup. Maybe they\\'re in a kitchen or a living room. The sink is probably near the kitchen, and the TV is in the living area. So, Person A is at the sink, and Person B is at the TV. \\n\\nWhy does the first response say \"The Sinking Guy\" and \"The TV Guy\"? Maybe it\\'s a play on words or nicknames. \"Sinking Guy\" could imply that he\\'s always at the sink, like his name is \"Sinking'}]\n",
            "{'role': 'assistant', 'content': 'Okay, so I need to figure out who\\'s at the sink and tell that name to the person at the TV stand. Hmm, the user gave a response that says the person at the sink is \"The Sinking Guy\" and the person at the TV is \"The TV Guy.\" But wait, how does that make sense? If I\\'m supposed to tell the name at the sink to the person at the TV, shouldn\\'t the information flow from sink to TV?\\n\\nFirst, I should visualize the setup. Maybe they\\'re in a kitchen or a living room. The sink is probably near the kitchen, and the TV is in the living area. So, Person A is at the sink, and Person B is at the TV. \\n\\nWhy does the first response say \"The Sinking Guy\" and \"The TV Guy\"? Maybe it\\'s a play on words or nicknames. \"Sinking Guy\" could imply that he\\'s always at the sink, like his name is \"Sinking'}\n",
            "Completion: Okay, so I need to figure out who's at the sink and tell that name to the person at the TV stand. Hmm, the user gave a response that says the person at the sink is \"The Sinking Guy\" and the person at the TV is \"The TV Guy.\" But wait, how does that make sense? If I'm supposed to tell the name at the sink to the person at the TV, shouldn't the information flow from sink to TV?\n",
            "\n",
            "First, I should visualize the setup. Maybe they're in a kitchen or a living room. The sink is probably near the kitchen, and the TV is in the living area. So, Person A is at the sink, and Person B is at the TV. \n",
            "\n",
            "Why does the first response say \"The Sinking Guy\" and \"The TV Guy\"? Maybe it's a play on words or nicknames. \"Sinking Guy\" could imply that he's always at the sink, like his name is \"Sinking, Answer: ['{\"commands\":[{\"action\":\"go\",\"complement\":\"sink\",\"characteristic\":\"\"},{\"action\":\"find_person\",\"complement\":\"\",\"characteristic\":\"\"},{\"action\":\"find_person_info\",\"complement\":\"name\",\"characteristic\":\"\"},{\"action\":\"go\",\"complement\":\"tv stand\",\"characteristic\":\"\"},{\"action\":\"find_person\",\"complement\":\"\",\"characteristic\":\"\"},{\"action\":\"contextual_say\",\"complement\":\"tell the name of the person at the sink to the person at the tv stand\",\"characteristic\":\"find_person_info\"}]}', '{\"commands\":[{\"action\":\"go\",\"complement\":\"sink\",\"characteristic\":\"\"},{\"action\":\"find_person\",\"complement\":\"\",\"characteristic\":\"\"},{\"action\":\"find_person_info\",\"complement\":\"name\",\"characteristic\":\"\"},{\"action\":\"go\",\"complement\":\"tv stand\",\"characteristic\":\"\"},{\"action\":\"find_person\",\"complement\":\"\",\"characteristic\":\"\"},{\"action\":\"contextual_say\",\"complement\":\"tell the name of the person at the sink to the person at the tv stand\",\"characteristic\":\"find_person_info\"}]}', '{\"commands\":[{\"action\":\"go\",\"complement\":\"sink\",\"characteristic\":\"\"},{\"action\":\"find_person\",\"complement\":\"\",\"characteristic\":\"\"},{\"action\":\"find_person_info\",\"complement\":\"name\",\"characteristic\":\"\"},{\"action\":\"go\",\"complement\":\"tv stand\",\"characteristic\":\"\"},{\"action\":\"find_person\",\"complement\":\"\",\"characteristic\":\"\"},{\"action\":\"contextual_say\",\"complement\":\"tell the name of the person at the sink to the person at the tv stand\",\"characteristic\":\"find_person_info\"}]}', '{\"commands\":[{\"action\":\"go\",\"complement\":\"sink\",\"characteristic\":\"\"},{\"action\":\"find_person\",\"complement\":\"\",\"characteristic\":\"\"},{\"action\":\"find_person_info\",\"complement\":\"name\",\"characteristic\":\"\"},{\"action\":\"go\",\"complement\":\"tv stand\",\"characteristic\":\"\"},{\"action\":\"find_person\",\"complement\":\"\",\"characteristic\":\"\"},{\"action\":\"contextual_say\",\"complement\":\"tell the name of the person at the sink to the person at the tv stand\",\"characteristic\":\"find_person_info\"}]}', '{\"commands\":[{\"action\":\"go\",\"complement\":\"sink\",\"characteristic\":\"\"},{\"action\":\"find_person\",\"complement\":\"\",\"characteristic\":\"\"},{\"action\":\"find_person_info\",\"complement\":\"name\",\"characteristic\":\"\"},{\"action\":\"go\",\"complement\":\"tv stand\",\"characteristic\":\"\"},{\"action\":\"find_person\",\"complement\":\"\",\"characteristic\":\"\"},{\"action\":\"contextual_say\",\"complement\":\"tell the name of the person at the sink to the person at the tv stand\",\"characteristic\":\"find_person_info\"}]}', '{\"commands\":[{\"action\":\"go\",\"complement\":\"sink\",\"characteristic\":\"\"},{\"action\":\"find_person\",\"complement\":\"\",\"characteristic\":\"\"},{\"action\":\"find_person_info\",\"complement\":\"name\",\"characteristic\":\"\"},{\"action\":\"go\",\"complement\":\"tv stand\",\"characteristic\":\"\"},{\"action\":\"find_person\",\"complement\":\"\",\"characteristic\":\"\"},{\"action\":\"contextual_say\",\"complement\":\"tell the name of the person at the sink to the person at the tv stand\",\"characteristic\":\"find_person_info\"}]}']\n"
          ]
        }
      ],
      "source": [
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "        # response_shape_reward_func,\n",
        "        # soft_response_shape_reward_func,\n",
        "        # valid_action_reward_func,\n",
        "        # characteristics_complements_reward_func,\n",
        "        correctness_reward_func,\n",
        "        response_shape_reward_func,\n",
        "        soft_response_shape_reward_func,\n",
        "        valid_action_reward_func,\n",
        "        characteristics_complements_reward_func,\n",
        "        strict_xml_format_reward_func,\n",
        "        soft_xml_format_reward_func,\n",
        "        xmlcount_reward_func,\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = dataset,\n",
        ")\n",
        "try:\n",
        "  trainer.train(resume_from_checkpoint = True)\n",
        "except Exception as e:\n",
        "  # Likely because of no checkpoints available\n",
        "  print(e)\n",
        "  trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5OTFFHKusH0"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Now let's try the model we just trained! First, let's first try the model without any GRPO trained:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2rHOjeiusH0"
      },
      "outputs": [],
      "source": [
        "text = tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"user\", \"content\" : \"Calculate pi.\"},\n",
        "], tokenize = False, add_generation_prompt = True)\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 0.8,\n",
        "    top_p = 0.95,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    [text],\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = None,\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQiK7CzqusH0"
      },
      "outputs": [],
      "source": [
        "dataset['question']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp2HbMIxusH0"
      },
      "source": [
        "And now with the LoRA we just trained with GRPO - we first save the LoRA first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Enq7N1uusH0"
      },
      "outputs": [],
      "source": [
        "model.save_lora(\"grpo_saved_lora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTYZ2gHbusH1"
      },
      "source": [
        "Now we load the LoRA and test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ItbeErVusH1"
      },
      "outputs": [],
      "source": [
        "text = tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
        "    {\"role\" : \"user\", \"content\" : \"Calculate pi.\"},\n",
        "], tokenize = False, add_generation_prompt = True)\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 0.8,\n",
        "    top_p = 0.95,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    text,\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q5jlR-vusH1"
      },
      "source": [
        "Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXDNRCbkusH1"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7snJGbZkusH1"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFjyZTSrusH1"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxKx35hAusH1"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if True: model.push_to_hub_gguf(\"diegohc/finetuning-tests\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLuSPQP3usH1"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Llama 3.2 Conversational notebook. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0db82db68e98427fbd72972466ff9cb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c624c36326e446c9ab51636e7cdd367e",
              "IPY_MODEL_f03795c7b29b4acebee3a5dfa10203fb",
              "IPY_MODEL_2a6cd2dec0414932b6a96bda6308addf"
            ],
            "layout": "IPY_MODEL_dbdf6355223b4a9e9d8fb337e3ed21d9"
          }
        },
        "c624c36326e446c9ab51636e7cdd367e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ca74c78e2fe4f5b88edd50d9bfd6a7d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_85bf2cbe3ff641c9807b77a58b8f8421",
            "value": ""
          }
        },
        "f03795c7b29b4acebee3a5dfa10203fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c05aaae75454cbfb5e3648184279c6a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6759f8d9e80047648239d68ff0ac7e86",
            "value": 1
          }
        },
        "2a6cd2dec0414932b6a96bda6308addf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f035675430bc4d14baf7aaa4f2538f48",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_88106b043adc458fae62eeac1297024b",
            "value": "Loading‚Äásafetensors‚Äácheckpoint‚Äáshards:‚Äá100%‚ÄáCompleted‚Äá|‚Äá1/1‚Äá[00:29&lt;00:00,‚Äá29.76s/it]\n"
          }
        },
        "dbdf6355223b4a9e9d8fb337e3ed21d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ca74c78e2fe4f5b88edd50d9bfd6a7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85bf2cbe3ff641c9807b77a58b8f8421": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c05aaae75454cbfb5e3648184279c6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6759f8d9e80047648239d68ff0ac7e86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f035675430bc4d14baf7aaa4f2538f48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88106b043adc458fae62eeac1297024b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be9b3764974a44a6b601c072bb29fb4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e33850e2b8641658cb733a25a014e8d",
              "IPY_MODEL_18842838577b4a478541c4a5b341382e",
              "IPY_MODEL_afba8e2a79914f7bbfab4fedaee2d66a"
            ],
            "layout": "IPY_MODEL_cbfed2dc5d524033a4c0d3bdfc3f139b"
          }
        },
        "6e33850e2b8641658cb733a25a014e8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b285c20d1aed4ddca49a663c443f8dca",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_da9db8ebc10341f1a8b6e8130bc858b5",
            "value": ""
          }
        },
        "18842838577b4a478541c4a5b341382e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8ee07eded474da895259026b6aaeaab",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_df95a5f70c6a47999a2f2cfe4545a344",
            "value": 1
          }
        },
        "afba8e2a79914f7bbfab4fedaee2d66a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5800b183c7554b3b98b0c3de173054a7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8f7e42a3c34a4a5498d099f623014797",
            "value": "Loading‚Äásafetensors‚Äácheckpoint‚Äáshards:‚Äá100%‚ÄáCompleted‚Äá|‚Äá1/1‚Äá[00:26&lt;00:00,‚Äá26.61s/it]\n"
          }
        },
        "cbfed2dc5d524033a4c0d3bdfc3f139b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b285c20d1aed4ddca49a663c443f8dca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da9db8ebc10341f1a8b6e8130bc858b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8ee07eded474da895259026b6aaeaab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df95a5f70c6a47999a2f2cfe4545a344": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5800b183c7554b3b98b0c3de173054a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f7e42a3c34a4a5498d099f623014797": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2715e16f4ac44df9b890ad26c3c1187c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dbeae62d2b8948e1b70e2a5a9cd78b79",
              "IPY_MODEL_12f9621bb9e24d0297567a2fbf5e6997",
              "IPY_MODEL_452eebfa3d31418db25ba889ed6b692d"
            ],
            "layout": "IPY_MODEL_32b6e0b6c4424ef299416d9b6ca28b75"
          }
        },
        "dbeae62d2b8948e1b70e2a5a9cd78b79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58f7337960584621855d4f511a51a67f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fa3b1d1fa89044a3aa9fcadcfa6e6ac0",
            "value": "Capturing‚ÄáCUDA‚Äágraph‚Äáshapes:‚Äá100%"
          }
        },
        "12f9621bb9e24d0297567a2fbf5e6997": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aae1d02679ef4f61836077d1d88d547d",
            "max": 23,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0bf68822ad8b4f01a9170cf6a02ec2ad",
            "value": 23
          }
        },
        "452eebfa3d31418db25ba889ed6b692d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62b21353a533439b8e01873c80e1d1c9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_71f7c930023348dc84d1cb63069b0f72",
            "value": "‚Äá23/23‚Äá[01:07&lt;00:00,‚Äá‚Äá4.15s/it]"
          }
        },
        "32b6e0b6c4424ef299416d9b6ca28b75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58f7337960584621855d4f511a51a67f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa3b1d1fa89044a3aa9fcadcfa6e6ac0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aae1d02679ef4f61836077d1d88d547d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bf68822ad8b4f01a9170cf6a02ec2ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "62b21353a533439b8e01873c80e1d1c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71f7c930023348dc84d1cb63069b0f72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}