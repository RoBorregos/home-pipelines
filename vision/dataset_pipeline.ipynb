{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/RoBorregos/home-pipelines/blob/vision-train/dataset_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7o1d_Y4usqzS",
    "outputId": "9636ee85-7a1a-4008-cf75-6f740852cc3d"
   },
   "outputs": [],
   "source": [
    "%%writefile /content/requirements.txt\n",
    "numpy\n",
    "opencv-python\n",
    "pillow\n",
    "pycocotools\n",
    "pyyaml\n",
    "torch\n",
    "ultralytics\n",
    "matplotlib\n",
    "imutils\n",
    "argparse\n",
    "groundingdino-py\n",
    "segment-anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8BwpM1sG-6vw",
    "outputId": "521e27fb-c760-48eb-dff4-16db0ee4a618"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OFGYuTSq-c5M",
    "outputId": "1fc15819-4fb9-4e5a-d596-f99c297f7a2e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw, ImageEnhance, ImageFilter, ImageFont, UnidentifiedImageError\n",
    "from pycocotools import mask\n",
    "import json\n",
    "import yaml\n",
    "import csv\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import ultralytics\n",
    "import time\n",
    "import imutils\n",
    "import argparse\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "import groundingdino.datasets.transforms as T\n",
    "from groundingdino.models import build_model\n",
    "from groundingdino.util import box_ops\n",
    "from groundingdino.util.inference import load_model, predict\n",
    "from groundingdino.util.slconfig import SLConfig\n",
    "from groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "from groundingdino.util.vl_utils import create_positive_map_from_span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5oN4FDoBSwW"
   },
   "source": [
    "# Attention!\n",
    "If you have specific pictures, place the folders in ./images and skip the next two blocks of code which download a default dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kbz7bnpTIx6a",
    "outputId": "5f332b6c-1540-4639-be7c-b78149581f61"
   },
   "outputs": [],
   "source": [
    "!pip install kagglehub\n",
    "import kagglehub\n",
    "\n",
    "dataset_path = kagglehub.dataset_download(\"bhavikjikadara/dog-and-cat-classification-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V38GLZoAAnGw",
    "outputId": "fcfa7c1e-0b33-48ae-f7e6-d733c147b5f6"
   },
   "outputs": [],
   "source": [
    "!cp -r {dataset_path}/PetImages /content\n",
    "!mv /content/PetImages /content/images\n",
    "\n",
    "def clean_and_trim_dataset(folder):\n",
    "    total_removed = 0\n",
    "\n",
    "    for category in [\"Cat\", \"Dog\"]:\n",
    "        path = os.path.join(folder, category)\n",
    "        valid_images = []\n",
    "\n",
    "        # Step 1: Remove corrupt images\n",
    "        for img_name in os.listdir(path):\n",
    "            img_path = os.path.join(path, img_name)\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    img.verify()\n",
    "                valid_images.append(img_path)\n",
    "            except (UnidentifiedImageError, OSError):\n",
    "                os.remove(img_path)\n",
    "                total_removed += 1\n",
    "\n",
    "        print(f\"Removed {total_removed} corrupt images from {category}\")\n",
    "\n",
    "        # Step 2: Remove part of the remaining valid images\n",
    "        to_delete = random.sample(valid_images, len(valid_images) * 99 // 100)\n",
    "        for img_path in to_delete:\n",
    "            os.remove(img_path)\n",
    "\n",
    "        print(f\"Removed {len(to_delete)} images from {category} to reduce dataset size\")\n",
    "\n",
    "clean_and_trim_dataset(\"/content/images\")\n",
    "\n",
    "def count_files_in_dir(directory):\n",
    "    # List all files (images) in the directory and count them\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    return len(files)\n",
    "\n",
    "# Example usage for \"Cat\" and \"Dog\" folders\n",
    "cat_folder = \"/content/images/Cat\"\n",
    "dog_folder = \"/content/images/Dog\"\n",
    "\n",
    "print(f\"Number of cat images: {count_files_in_dir(cat_folder)}\")\n",
    "print(f\"Number of dog images: {count_files_in_dir(dog_folder)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W_q0rbC8z1np",
    "outputId": "c8ac3f35-1730-4493-d455-c878a9584793"
   },
   "outputs": [],
   "source": [
    "base_path = \"./images\"\n",
    "size = 720\n",
    "\n",
    "for class_dir in os.listdir(base_path):\n",
    "    class_path = os.path.join(base_path, class_dir)\n",
    "\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    for filename in os.listdir(class_path):\n",
    "        file_path = os.path.join(class_path, filename)\n",
    "\n",
    "        try:\n",
    "            with Image.open(file_path) as img:\n",
    "                img = img.resize((size, size))\n",
    "                img.save(file_path)\n",
    "        except (UnidentifiedImageError, OSError) as e:\n",
    "            print(f\"Removing corrupt file: {file_path}\")\n",
    "            os.remove(file_path)  # delete corrupt image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ldTlMrhBbZ1m",
    "outputId": "5859ed7e-667f-4d92-e9b7-b6b05ad6fb65"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
    "!wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n",
    "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d5b0ba020b5849b8a437165237968fa6",
      "2e2dffa5e19f402fbbc6109d1d52783d",
      "6f93bf755bcf4806ae05a5ce115c6ed9",
      "8561b858d2084fb5b8fbc71b518ffbaa",
      "8d1cd9c322474acaa6461f006b7354b8",
      "3770f7c9ab474bf4a46e02505a194481",
      "1b20d2dfe93244afa5e603bb5461dd21",
      "d3e4add9b4774c40ae69db9db6089e57",
      "3e51e555684e4c088442c9c4539b8a83",
      "2a698c8b07ee43a1a523dd1de489db17",
      "557f27be3a84425195a536c82ba3f767",
      "5db3447dfccf484eb689b6d50db6cbbf",
      "3c84890e7fed4c0e8e43209e643e935d",
      "7fd8b260862b426fbd9894d19a4d3f78",
      "7f16d6db51544e19b523159e08c36668",
      "e1fd2c746f0b4cb9a13e9525af931446",
      "3bd3ad8d2cdd49abad2db607cfe69bf0",
      "86e997475c124902b70c0d63f05860e0",
      "dbbf264c370c4297b9e61209b8ae5bbe",
      "04aa7a2bbd2b4b2f941e19f6e574b9ab",
      "2982e63de743475eaf3454216407341b",
      "918a8b612077485a8dbccf8515f80e62",
      "0206870f85ca4fed8b96d9e5c0aeacc4",
      "9b8830e36aab4d5bbe29188cb6d961e0",
      "eb6269db193949e5aaed659c45ed7486",
      "be8f8ef9b6a94e2a90deba7fb7e421c2",
      "a52480eb851c47a9a7a177b457a61910",
      "1ef39d0f29974d3197ffd221e481eff1",
      "e40fc511f02646b38012d22349de3c78",
      "d2b1400b647441229141157541016dba",
      "6a084e3c1a63480bb30429d3b2eba6d0",
      "0c85d523498741f19b60a762514ccc22",
      "685eb4bb2088473285608bf1eda71eea",
      "c27f1b4a94bb471aa0b30d093d526ae2",
      "516a32969fd74cbd814330b3c0314a71",
      "9488bc2ea5b3485aa35ad2a7c0c46246",
      "e99ec0d230014c37ae3ebbc29bdba047",
      "c47f3a3704a248f587fcf8a47f2cc150",
      "3999c84f353545259dcb881843b3353e",
      "c7723f6988994cbbb22a3e0b1e019dc2",
      "4fd6f4f588f442b8b5932d3accd4fd67",
      "d0b211d54c7c4edb8fbd477ede0f270b",
      "88a3a9e277fa4f2fbe314242ce33ab5d",
      "442fa0a205124a54a0dbed7832aaf83b",
      "e4a18424879a4184860c35cbc80d2ca2",
      "9386a1cd2b244aec84c4d6c45d926a89",
      "971497ea9a67408f95b97816e3e7f2d7",
      "4bb0b05b7dfc440db4ce10a96eb209df",
      "5fca093eaa87416bbbfaf7f34b7eca42",
      "42eabfdffd2a4889b2057ba91d51855b",
      "2cda735327e84c659f0f5352b9992115",
      "08929d7332ad4d5e98341c6df3f59c9d",
      "b0e609e1a08247e69f3107ee595cb6bd",
      "a0d1bc6c67704072b041db3dd2e369a5",
      "84f5605d7bd146e88340d2e95b1d69ed"
     ]
    },
    "id": "2-G0iL0HJywT",
    "outputId": "03cf6c0d-4e13-485a-86b2-cb37a5370e28"
   },
   "outputs": [],
   "source": [
    "SAVE_BB = False\n",
    "results_path = \"/content/processed\" #path to save results already processed and segmented images\n",
    "config_file = \"/content/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"  # change the path of the model config file\n",
    "checkpoint_path = \"/content/groundingdino_swint_ogc.pth\"  # change the path of the model\n",
    "output_dir = results_path\n",
    "box_threshold = 0.3\n",
    "text_threshold = 0.25\n",
    "token_spans = None\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "sam_model = \"h\"\n",
    "\n",
    "#use sam model\n",
    "#wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
    "#wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\n",
    "if sam_model ==\"h\":\n",
    "  sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "  model_type = \"vit_h\"\n",
    "else:\n",
    "  sam_checkpoint = \"sam_vit_l_0b3195.pth\"\n",
    "  model_type = \"vit_l\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using\", device)\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "images=[]\n",
    "annotations=[]\n",
    "categories=[]\n",
    "\n",
    "img_id=0\n",
    "anno_id=0\n",
    "\n",
    "# Make a list of all the directories in the path\n",
    "base_path = \"./images\"\n",
    "path_to_classes = [f.path for f in os.scandir(base_path) if f.is_dir()]\n",
    "\n",
    "def load_image(image_path):\n",
    "\n",
    "    image_pil = Image.open(image_path).convert(\"RGB\")  # load image\n",
    "\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.RandomResize([800], max_size=1333),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    image, _ = transform(image_pil, None)  # 3, h, w\n",
    "    return image_pil, image\n",
    "\n",
    "\n",
    "def load_model(model_config_path, model_checkpoint_path, cpu_only=False):\n",
    "    args = SLConfig.fromfile(model_config_path)\n",
    "    args.device = device\n",
    "    model = build_model(args)\n",
    "    checkpoint = torch.load(model_checkpoint_path, map_location=\"cpu\")\n",
    "    load_res = model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n",
    "    print(load_res)\n",
    "    _ = model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_grounding_output(model, image, caption, box_threshold, text_threshold=None, with_logits=True, cpu_only=False, token_spans=None):\n",
    "    assert text_threshold is not None or token_spans is not None, \"text_threshould and token_spans should not be None at the same time!\"\n",
    "    caption = caption.lower()\n",
    "    caption = caption.strip()\n",
    "    if not caption.endswith(\".\"):\n",
    "        caption = caption + \".\"\n",
    "    model = model.to(device)\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        print(\"Running model...\")\n",
    "        outputs = model(image[None], captions=[caption])\n",
    "    logits = outputs[\"pred_logits\"].sigmoid()[0]  # (nq, 256)\n",
    "    boxes = outputs[\"pred_boxes\"][0]  # (nq, 4)\n",
    "\n",
    "    # filter output\n",
    "    if token_spans is None:\n",
    "        logits_filt = logits.cpu().clone()\n",
    "        boxes_filt = boxes.cpu().clone()\n",
    "        filt_mask = logits_filt.max(dim=1)[0] > box_threshold\n",
    "        logits_filt = logits_filt[filt_mask]  # num_filt, 256\n",
    "        boxes_filt = boxes_filt[filt_mask]  # num_filt, 4\n",
    "\n",
    "        # get phrase\n",
    "        tokenlizer = model.tokenizer\n",
    "        tokenized = tokenlizer(caption)\n",
    "        # build pred\n",
    "        pred_phrases = []\n",
    "        for logit, box in zip(logits_filt, boxes_filt):\n",
    "            pred_phrase = get_phrases_from_posmap(logit > text_threshold, tokenized, tokenlizer)\n",
    "            if with_logits:\n",
    "                pred_phrases.append(pred_phrase + f\"({str(logit.max().item())[:4]})\")\n",
    "            else:\n",
    "                pred_phrases.append(pred_phrase)\n",
    "    else:\n",
    "        # given-phrase mode\n",
    "        positive_maps = create_positive_map_from_span(\n",
    "            model.tokenizer(text_prompt),\n",
    "            token_span=token_spans\n",
    "        ).to(image.device) # n_phrase, 256\n",
    "\n",
    "        logits_for_phrases = positive_maps @ logits.T # n_phrase, nq\n",
    "        all_logits = []\n",
    "        all_phrases = []\n",
    "        all_boxes = []\n",
    "        for (token_span, logit_phr) in zip(token_spans, logits_for_phrases):\n",
    "            # get phrase\n",
    "            phrase = ' '.join([caption[_s:_e] for (_s, _e) in token_span])\n",
    "            # get mask\n",
    "            filt_mask = logit_phr > box_threshold\n",
    "            # filt box\n",
    "            all_boxes.append(boxes[filt_mask])\n",
    "            # filt logits\n",
    "            all_logits.append(logit_phr[filt_mask])\n",
    "            if with_logits:\n",
    "                logit_phr_num = logit_phr[filt_mask]\n",
    "                all_phrases.extend([phrase + f\"({str(logit.item())[:4]})\" for logit in logit_phr_num])\n",
    "            else:\n",
    "                all_phrases.extend([phrase for _ in range(len(filt_mask))])\n",
    "        boxes_filt = torch.cat(all_boxes, dim=0).cpu()\n",
    "        pred_phrases = all_phrases\n",
    "\n",
    "\n",
    "    return boxes_filt, pred_phrases\n",
    "\n",
    "\n",
    "def verify_or_create_dir(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    print(f\"Verified/created: {path}\")\n",
    "\n",
    "def count_all_files_in_dir(directory):\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        count += len([f for f in files if os.path.isfile(os.path.join(root, f))])\n",
    "    return count\n",
    "\n",
    "# Get total number of images to process\n",
    "image_dir = \"/content/images\"\n",
    "number_of_images = count_all_files_in_dir(image_dir)\n",
    "print(f\"Total image files: {number_of_images}\")\n",
    "\n",
    "# Check if results directory exists, else create it\n",
    "verify_or_create_dir(results_path)\n",
    "\n",
    "# Main loop\n",
    "for class_path in path_to_classes:\n",
    "    imgPaths = os.listdir(class_path)\n",
    "    i = 0\n",
    "    if SAVE_BB:\n",
    "        class_name = os.path.basename(class_path)\n",
    "        verify_or_create_dir(f\"{results_path}/bbs/{class_name}\")\n",
    "\n",
    "    for imgPath in imgPaths:\n",
    "        # print(f\"Processing image: {imgPath}\")\n",
    "        print(f\"%{i * 100 / number_of_images}\")\n",
    "        img = imutils.resize(cv2.imread(f\"{class_path}/{imgPath}\"))\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "    #------------------------start grounding----------------------------------------------\n",
    "\n",
    "        # Image_path = args.image_path\n",
    "        cpu_only = False if torch.cuda.is_available() else True\n",
    "\n",
    "        # Load image\n",
    "        image_pil, image = load_image(f\"{class_path}/{imgPath}\")\n",
    "\n",
    "        # Load model\n",
    "        model = load_model(config_file, checkpoint_path, cpu_only=cpu_only)\n",
    "\n",
    "        # Set the text_threshold to None if token_spans is set.\n",
    "        if token_spans is not None:\n",
    "            text_threshold = None\n",
    "            print(\"Using token_spans. Set the text_threshold to None.\")\n",
    "\n",
    "        # Run model\n",
    "        text_prompt = os.path.basename(class_path)\n",
    "        boxes_filt, pred_phrases = get_grounding_output(\n",
    "            model, image, text_prompt, box_threshold, text_threshold, cpu_only=cpu_only, token_spans=eval(f\"{token_spans}\")\n",
    "        )\n",
    "\n",
    "        # Found bb dimensions\n",
    "\n",
    "        size = image_pil.size\n",
    "        pred_dict = {\n",
    "            \"boxes\": boxes_filt,\n",
    "            \"size\": [size[1], size[0]],  # H, W\n",
    "            \"labels\": pred_phrases,\n",
    "        }\n",
    "\n",
    "        H, W = pred_dict[\"size\"]\n",
    "        boxes = pred_dict[\"boxes\"]\n",
    "        labels = pred_dict[\"labels\"]\n",
    "        assert len(boxes) == len(labels), \"boxes and labels must have same length\"\n",
    "\n",
    "        draw = ImageDraw.Draw(image_pil)\n",
    "        mask = Image.new(\"L\", image_pil.size, 0)\n",
    "        mask_draw = ImageDraw.Draw(mask)\n",
    "\n",
    "        #change pil image to cv2 image\n",
    "        img = cv2.cvtColor(np.array(image_pil), cv2.COLOR_RGB2BGR)\n",
    "        img2 = img.copy()\n",
    "        # draw boxes and masks\n",
    "        for box, label in zip(boxes, labels):\n",
    "            # from 0..1 to 0..W, 0..H\n",
    "            box = box * torch.Tensor([W, H, W, H])\n",
    "            # from xywh to xyxy\n",
    "            box[:2] -= box[2:] / 2\n",
    "            box[2:] += box[:2]\n",
    "            # random color\n",
    "            color = tuple(np.random.randint(0, 255, size=1).tolist())\n",
    "            # draw\n",
    "            padding = 10\n",
    "            x0, y0, x1, y1 = box\n",
    "            x0, y0, x1, y1 = int(x0)-padding, int(y0)-padding, int(x1)+padding, int(y1)+padding\n",
    "\n",
    "            #validate if the bounding box is inside the image\n",
    "            if x0 < 0:\n",
    "                x0 = 0\n",
    "            if y0 < 0:\n",
    "                y0 = 0\n",
    "            if x1 > W:\n",
    "                x1 = W\n",
    "            if y1 > H:\n",
    "                y1 = H\n",
    "\n",
    "            #draw rectangles\n",
    "            cv2.rectangle(img2, (x0, y0), (x1, y1), color, 2)\n",
    "\n",
    "            draw.rectangle([x0, y0, x1, y1], outline=color, width=6)\n",
    "            # draw.text((x0, y0), str(label), fill=color)\n",
    "\n",
    "            font = ImageFont.load_default()\n",
    "            if hasattr(font, \"getbbox\"):\n",
    "                bbox = draw.textbbox((x0, y0), str(label), font)\n",
    "            else:\n",
    "                w, h = draw.textsize(str(label), font)\n",
    "                bbox = (x0, y0, w + x0, y0 + h)\n",
    "            # bbox = draw.textbbox((x0, y0), str(label))\n",
    "            draw.rectangle(bbox, fill=color)\n",
    "            draw.text((x0, y0), str(label), fill=\"white\")\n",
    "\n",
    "            mask_draw.rectangle([x0, y0, x1, y1], fill=255, width=6)\n",
    "\n",
    "    # ----------------Start SAM--------------------------------------------------------------\n",
    "\n",
    "            class_name = class_path.split(\"/\")[-1]\n",
    "            sam_bounding_box = np.array([x0, y0, x1, y1])\n",
    "            ran_sam = False\n",
    "            #run sam\n",
    "            if ran_sam == False:\n",
    "                predictor.set_image(img)\n",
    "                ran_sam = True\n",
    "\n",
    "            mask, _, _ = predictor.predict(\n",
    "                point_coords=None,\n",
    "                point_labels=None,\n",
    "                box=sam_bounding_box,\n",
    "                multimask_output=False,\n",
    "            )\n",
    "\n",
    "            mask, _, _ = predictor.predict(box=sam_bounding_box, multimask_output=False)\n",
    "\n",
    "            # Make png mask\n",
    "            contours, _ = cv2.findContours(mask[0].astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) # Your call to find the contours\n",
    "\n",
    "            # threshold input image using otsu thresholding as mask and refine with morphology\n",
    "            ret, pngmask = cv2.threshold(mask[0].astype(np.uint8), 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "            kernel = np.ones((9,9), np.uint8)\n",
    "            pngmask = cv2.morphologyEx(pngmask, cv2.MORPH_CLOSE, kernel)\n",
    "            pngmask = cv2.morphologyEx(pngmask, cv2.MORPH_OPEN, kernel)\n",
    "            result = img.copy()\n",
    "            result = cv2.cvtColor(result, cv2.COLOR_BGR2BGRA)\n",
    "            result[:, :, 3] = pngmask\n",
    "\n",
    "    # ----------------End SAM-----------------------------------------------------------------\n",
    "\n",
    "            if SAVE_BB:\n",
    "              cv2.imwrite(f\"{results_path}/bbs/{class_name}/{imgPath}\", img2)\n",
    "\n",
    "            verify_or_create_dir(f\"{results_path}/{class_name}\")\n",
    "\n",
    "            file_path = f\"{results_path}/{class_name}/{imgPath[:-4]}.png\"\n",
    "            if os.path.exists(file_path):\n",
    "                if os.path.exists(f\"{results_path}/{class_name}/{imgPath[:-4]}_1.png\"):\n",
    "                    # print(\"File already exists, saving with _2\")\n",
    "                    cv2.imwrite(f\"{results_path}/{class_name}/{imgPath[:-4]}_2.png\", result)\n",
    "                else:\n",
    "                    # print(\"File already exists, saving with _1\")\n",
    "                    file_path = f\"{results_path}/{class_name}/{imgPath[:-4]}_1.png\"\n",
    "\n",
    "            cv2.imwrite(file_path, result)\n",
    "            i = i + 1\n",
    "            ran_sam = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MEhhdSJp8ICk",
    "outputId": "638fa930-9a26-49c2-c7a8-e74bf1346fd8"
   },
   "outputs": [],
   "source": [
    "results_path = \"./DS_res/\"\n",
    "path_to_classes = [f.path for f in os.scandir(\"./processed\") if f.is_dir()]\n",
    "\n",
    "for class_path in path_to_classes:\n",
    "    class_name = os.path.basename(class_path)\n",
    "    verify_or_create_dir(results_path + class_name)\n",
    "    for file_name in os.listdir(class_path):\n",
    "        try:\n",
    "            file_path = class_path + \"/\" + file_name\n",
    "            print(f\"{file_path} started\")\n",
    "            my_image = Image.open(file_path)\n",
    "            black = Image.new('RGBA', my_image.size)\n",
    "            my_image = Image.composite(my_image, black, my_image)\n",
    "            #print(\"aqui\")\n",
    "            cropped_image = my_image.crop(my_image.getbbox())\n",
    "            cropped_image.save(f\"{results_path}{class_name}/{file_name}\")\n",
    "            print(f\"{file_name} done\")\n",
    "        except Exception as e:\n",
    "            print(f\"{file_name} failed {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rJpoRpAsEnD6",
    "outputId": "e0e1fb31-ef48-4dfb-8d89-e6743b14ae3d"
   },
   "outputs": [],
   "source": [
    "# Get all subfolders inside results_path\n",
    "path_to_classes = [f.path for f in os.scandir(results_path) if f.is_dir()]\n",
    "\n",
    "# Build list of (path, class_name) tuples\n",
    "fg_folders = [(path, os.path.basename(path)) for path in path_to_classes]\n",
    "\n",
    "# Define folders\n",
    "bg_folder = \"./bg/\"\n",
    "verify_or_create_dir(bg_folder)\n",
    "output_folder = \"./ds_final/\" # TODO: CHANGE TO ABSOLUTE PATH FOR DATA.YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oguz8YWLInMU",
    "outputId": "d2f22a1e-aea4-4faf-c613-9d782e700efe"
   },
   "outputs": [],
   "source": [
    "objects_list = [os.path.basename(class_path) for class_path in path_to_classes]\n",
    "annotations_ID = {}\n",
    "categories = []\n",
    "for i, object in enumerate(objects_list):\n",
    "    annotations_ID[object] = i\n",
    "    categories.append({\"id\": i, \"name\": object})\n",
    "\n",
    "print(annotations_ID)\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZLBmi4rJO4_"
   },
   "outputs": [],
   "source": [
    "# Load the list of files in each of the three folders\n",
    "fg_files = {}\n",
    "for folder, category in fg_folders:\n",
    "    fg_files[category] = os.listdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C6rWuxxEJeAd",
    "outputId": "d1f0c65e-5a28-40f0-d792-92d4ec108288"
   },
   "outputs": [],
   "source": [
    "# Define the folder structure\n",
    "subfolders = [\n",
    "    \"train/images\",\n",
    "    \"train/labels\",\n",
    "    \"test/images\",\n",
    "    \"test/labels\",\n",
    "    \"valid/images\",\n",
    "    \"valid/labels\",\n",
    "]\n",
    "\n",
    "# Create them\n",
    "for sub in subfolders:\n",
    "    verify_or_create_dir(os.path.join(output_folder, sub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qc1o0hXINriN"
   },
   "source": [
    "# Segment\n",
    "\n",
    "NOT USED CURRENTLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOQdvVY5KlxX"
   },
   "outputs": [],
   "source": [
    "# THIS IS HARD CODED STILL\n",
    "import cv2\n",
    "\n",
    "# Load the image from disk\n",
    "image_path = \"./DS_res/Cat/10076.png\"\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Convert to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply thresholding\n",
    "_, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Find contours\n",
    "contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Draw contours on a copy of the original image\n",
    "contour_image = image.copy()\n",
    "cv2.drawContours(contour_image, contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "# Save the result\n",
    "output_path = \"./test.png\"\n",
    "cv2.imwrite(output_path, contour_image)\n",
    "print(f\"Saved contour image to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBOYwQpeLijp"
   },
   "source": [
    "# Autolabel\n",
    "\n",
    "For this step, add background images to ./bg\n",
    "\n",
    "You can run the next 2 following blocks to download and format a default dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JzHBMsQ0R6b0",
    "outputId": "51ff952e-2735-4224-e9b3-eeeecea04811"
   },
   "outputs": [],
   "source": [
    "!pip install kagglehub\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "dataset_path = kagglehub.dataset_download(\"balraj98/stanford-background-dataset\")\n",
    "\n",
    "os.system(f'cp \"{dataset_path}/images/\"* /content/bg/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RyD-Xx4qXLfY",
    "outputId": "82058f3c-f818-4723-fb61-2b3548593e01"
   },
   "outputs": [],
   "source": [
    "def clean_and_trim_dataset(folder):\n",
    "    total_removed = 0\n",
    "    valid_images = []\n",
    "\n",
    "    # Step 1: Remove corrupt images\n",
    "    for img_name in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, img_name)\n",
    "        if not os.path.isfile(img_path):\n",
    "            continue\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                img.verify()\n",
    "            valid_images.append(img_path)\n",
    "        except (UnidentifiedImageError, OSError):\n",
    "            os.remove(img_path)\n",
    "            total_removed += 1\n",
    "\n",
    "    print(f\"Removed {total_removed} corrupt images\")\n",
    "\n",
    "    # Step 2: Remove part of the remaining valid images\n",
    "    to_delete = random.sample(valid_images, len(valid_images) * 1 // 2)\n",
    "    for img_path in to_delete:\n",
    "        os.remove(img_path)\n",
    "\n",
    "    print(f\"Removed {len(to_delete)} images to reduce dataset size\")\n",
    "\n",
    "def count_files_in_dir(directory):\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    return len(files)\n",
    "\n",
    "# Apply on /content/bg\n",
    "clean_and_trim_dataset(\"/content/bg\")\n",
    "\n",
    "# Count remaining images\n",
    "print(f\"Remaining number of images: {count_files_in_dir('/content/bg')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TjBFmsTdQ-sZ"
   },
   "outputs": [],
   "source": [
    "!rm /content/ds_final/train/labels/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3DjdUzK7Lk9T"
   },
   "outputs": [],
   "source": [
    "images=[]\n",
    "annotations=[]\n",
    "annotations2=[]\n",
    "annot_csv=[]\n",
    "\n",
    "img_id=int(0)\n",
    "anno_id=int(0)\n",
    "\n",
    "rescaling_min = 0.20\n",
    "rescaling_max = 0.70\n",
    "\n",
    "# Ratios at which these values will be modified\n",
    "brightness_ratio = 0.05\n",
    "saturation_ratio = 0.05\n",
    "hue_ratio = 0.02\n",
    "\n",
    "trainfolder = output_folder + \"train/\"\n",
    "validfolder = output_folder + \"valid/\"\n",
    "\n",
    "images_to_generate = 500\n",
    "\n",
    "for j in range(images_to_generate):\n",
    "    #create empty label file\n",
    "    with open(f'{trainfolder}labels/{img_id}.txt', 'w') as file:\n",
    "        pass\n",
    "    #select hramdomly how many objects will be in an image\n",
    "    num_objects = random.randint(1, 3)\n",
    "    #print(\"number of objects\",num_objects)\n",
    "    # Select random foreground images from the three folders, with replacement\n",
    "    fg_categories = random.choices(objects_list, k=num_objects)\n",
    "\n",
    "    fg_files_selected = []\n",
    "    for category in fg_categories:\n",
    "        fg_files_selected.append([category,random.choice(fg_files[category])])\n",
    "    #print(\"seleccion\",fg_files_selected)\n",
    "    # Load the selected foreground images using Pillow\n",
    "    fg_imgs = []\n",
    "    for img in fg_files_selected:\n",
    "        folder = [f[0] for f in fg_folders if f[1] == img[0]][0]\n",
    "        fg_imgs.append([img[0],Image.open(folder + \"/\" + img[1]),folder+img[1]])\n",
    "\n",
    "    # Randomly resize and rotate the foreground images using Pillow's transform module\n",
    "    # img[0] is category, img[1] is image, img[2] is path\n",
    "    for img in fg_imgs:\n",
    "        fg_img=img[1]\n",
    "        angle = random.randint(-5, 5)\n",
    "        scale = random.uniform(rescaling_min, rescaling_max)\n",
    "        fg_img = fg_img.rotate(angle, resample=Image.BICUBIC, expand=True)\n",
    "        fg_img = fg_img.resize((int(fg_img.width * scale), int(fg_img.height * scale)))\n",
    "        fg_img = ImageEnhance.Brightness(fg_img).enhance(random.uniform(0.7, 1.3))\n",
    "        fg_img = ImageEnhance.Contrast(fg_img).enhance(random.uniform(0.9, 1.1))\n",
    "        fg_img = ImageEnhance.Color(fg_img).enhance(random.uniform(0.7, 1.3))\n",
    "        fg_img = fg_img.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.0, 0.5)))\n",
    "\n",
    "\n",
    "        img[1] = fg_img\n",
    "\n",
    "    # Load the background image using Pillow\n",
    "    bg_files = os.listdir(bg_folder)\n",
    "    bg_file = random.choice(bg_files)\n",
    "    bg_img = Image.open(bg_folder + bg_file)\n",
    "\n",
    "    # Define the maximum overlap as a percentage\n",
    "    max_overlap_pct = 10\n",
    "\n",
    "    # Define an array to keep track of occupied areas\n",
    "    occupied = np.zeros((bg_img.height, bg_img.width))\n",
    "\n",
    "    for img in fg_imgs:\n",
    "        fg_img=img[1]\n",
    "\n",
    "        # Calculate the maximum overlap area\n",
    "        max_overlap_area = (fg_img.width * fg_img.height)\n",
    "\n",
    "        seg_img = img[1]\n",
    "\n",
    "\n",
    "        # Convert the image to a NumPy array\n",
    "        img_arr = np.array(seg_img)\n",
    "        # Create a binary mask of the non-transparent pixels\n",
    "        mask = img_arr[:, :, 3] != 0\n",
    "\n",
    "        # Convert the mask to a COCO format segmentation\n",
    "        segmentation = []\n",
    "        for i in range(mask.shape[0]):\n",
    "            for j in range(mask.shape[1]):\n",
    "                if mask[i, j]:\n",
    "                    segmentation.append(j)\n",
    "                    segmentation.append(i)\n",
    "        segmentation = [segmentation]\n",
    "\n",
    "        # Calculate the area of the segmentation\n",
    "        area = 0\n",
    "        for i in range(len(segmentation[0]) // 2):\n",
    "            x1 = segmentation[0][2 * i]\n",
    "            y1 = segmentation[0][2 * i + 1]\n",
    "            x2 = segmentation[0][(2 * i + 2) % len(segmentation[0])]\n",
    "            y2 = segmentation[0][(2 * i + 3) % len(segmentation[0])]\n",
    "            area += x1 * y2 - x2 * y1\n",
    "        area = abs(area) / 2\n",
    "\n",
    "        # Draw the segmentation onto a copy of the original image\n",
    "        #image_copy = image.copy()\n",
    "        #cv2.fillPoly(image_copy, aux_segmentation, color=(0, 255, 0))\n",
    "\n",
    "        # Display the image with segmentation overlay\n",
    "        #cv2.imshow('Image with Segmentation', image_copy)\n",
    "        #cv2.waitKey(0)\n",
    "        #cv2.destroyAllWindows()\n",
    "\n",
    "        # Calculate the maximum allowed position for the top-left corner\n",
    "        max_x = bg_img.width - fg_img.width\n",
    "        max_y = bg_img.height - fg_img.height\n",
    "        area = fg_img.width * fg_img.height\n",
    "\n",
    "        # Generate a random location until an unoccupied area is found that meets the overlap limit\n",
    "        total_area = bg_img.width * bg_img.height\n",
    "        overlap_area = total_area\n",
    "\n",
    "        while overlap_area / area > max_overlap_pct / 100:\n",
    "            try:\n",
    "                x = random.randint(0, max_x)\n",
    "                y = random.randint(0, max_y)\n",
    "            except:\n",
    "                x = random.randint(0, abs(max_x))\n",
    "                y = random.randint(0, abs(max_y))\n",
    "\n",
    "            # Calculate the overlap area\n",
    "            overlap_area = np.sum(occupied[y:y+fg_img.height, x:x+fg_img.width])\n",
    "\n",
    "            # Check if the area is unoccupied and the overlap limit is not exceeded\n",
    "            if (max_overlap_area) >= overlap_area/10:\n",
    "                break\n",
    "            if i==10:\n",
    "                continue\n",
    "\n",
    "        for i in range(0, len(segmentation[0])):\n",
    "            if i % 2:\n",
    "                segmentation[0][i]=int(segmentation[0][i]+y)\n",
    "            else :\n",
    "                segmentation[0][i]=int(segmentation[0][i]+x)\n",
    "        # Update the occupied array\n",
    "        occupied[y:y+fg_img.height, x:x+fg_img.width] = 1\n",
    "\n",
    "        bg_img.paste(fg_img, (x, y), fg_img)\n",
    "        x_center_ann = (x+fg_img.width/2) / bg_img.width\n",
    "        y_center_ann = (y+fg_img.height/2) / bg_img.height\n",
    "        width_ann = fg_img.width / bg_img.width\n",
    "        height_ann = fg_img.height / bg_img.height\n",
    "        with open(f'{trainfolder}labels/{img_id}.txt', 'a') as f:\n",
    "            f.write(f\"{annotations_ID[img[0]]} {x_center_ann} {y_center_ann} {width_ann} {height_ann}\\n\")\n",
    "        annotations2.append({\"id\": anno_id,\"image_id\": img_id,\"category_id\": annotations_ID[img[0]],\"bbox\": [x, y, fg_img.width, fg_img.height],\"segmentation\": segmentation,\"area\": area,\"iscrowd\": 0})\n",
    "        annotations.append({\"id\": anno_id,\"image_id\": img_id,\"category_id\": annotations_ID[img[0]],\"bbox\": [x, y, fg_img.width, fg_img.height],\"segmentation\": [],\"area\": fg_img.height*fg_img.width,\"iscrowd\": 0})\n",
    "        annot_csv.append([\"TRAIN\", output_folder + str(img_id)+\".jpg\", img[0], x/bg_img.width, y/bg_img.height,\"\",\"\",(x+fg_img.width)/bg_img.width, (y+fg_img.height)/bg_img.height])\n",
    "        anno_id=anno_id+1\n",
    "        #draw = ImageDraw.Draw(bg_img)\n",
    "        #fdraw.rectangle((x, y, x+fg_img.width, y+fg_img.height), outline='red', width=3)\n",
    "    bg_img.save(f\"{trainfolder}images/\"+str(img_id)+\".jpg\", quality=100)\n",
    "    images.append({\"id\": img_id, \"file_name\": str(img_id)+\".jpg\",\"height\": bg_img.height,\"width\": bg_img.width})\n",
    "    img_id=img_id+1\n",
    "    #print(img_id)\n",
    "\n",
    "#making data.yaml\n",
    "data = dict(\n",
    "    train = f\"{trainfolder}images\",\n",
    "    val = f\"{validfolder}images\",\n",
    "    test = f\"{validfolder}images\",\n",
    "    nc = len(annotations_ID),\n",
    "    names = list(annotations_ID.keys())\n",
    "    )\n",
    "#storing\n",
    "with open(f'{output_folder}data.yaml', 'w') as outfile:\n",
    "    yaml.dump(data, outfile, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMY4iC9HM6Mm"
   },
   "source": [
    "# SplitTrainValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vczfmj5OM7Rl",
    "outputId": "ad2c9898-4708-424d-cfa9-35c836a33190"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "validation = 0.1\n",
    "test = 0.1\n",
    "\n",
    "# Assumes test has 100% of data\n",
    "output_folder = \"./ds_final/\"\n",
    "trainfolder = output_folder + \"train/\"\n",
    "trainfolderimgs = trainfolder + \"images/\"\n",
    "trainfolderlabels = trainfolder + \"labels/\"\n",
    "testfolder = output_folder + \"test/\"\n",
    "testfolderimgs = testfolder + \"images/\"\n",
    "testfolderlabels = testfolder + \"labels/\"\n",
    "validfolder = output_folder + \"valid/\"\n",
    "validfolderimgs = validfolder + \"images/\"\n",
    "validfolderlabels = validfolder + \"labels/\"\n",
    "\n",
    "fullSize = len(os.listdir(trainfolderimgs))\n",
    "validSize = int(fullSize * validation)\n",
    "testSize = int(fullSize * test)\n",
    "\n",
    "for i in range(validSize):\n",
    "    filelist = os.listdir(trainfolderimgs)\n",
    "    #randomize file list, to not pick files in order\n",
    "    random.shuffle(filelist)\n",
    "    filetomove = filelist[i]\n",
    "    #take out .jpg, .png, etc\n",
    "    filetomovename = filetomove[:-4]\n",
    "    #move images\n",
    "    shutil.move(f\"{trainfolderimgs}{filetomove}\", f\"{validfolderimgs}{filetomove}\")\n",
    "    #move labels\n",
    "    shutil.move(f\"{trainfolderlabels}{filetomovename}.txt\", f\"{validfolderlabels}{filetomovename}.txt\")\n",
    "for i in range(testSize):\n",
    "    filetomove = os.listdir(trainfolderimgs)[i]\n",
    "    #take out .jpg, .png, etc\n",
    "    filetomovename = filetomove[:-4]\n",
    "    #move images\n",
    "    shutil.move(f\"{trainfolderimgs}{filetomove}\", f\"{testfolderimgs}{filetomove}\")\n",
    "    #move labels\n",
    "    shutil.move(f\"{trainfolderlabels}{filetomovename}.txt\", f\"{testfolderlabels}{filetomovename}.txt\")\n",
    "\n",
    "#Validation\n",
    "print(f\"Train size is now: {len(os.listdir(trainfolderimgs))}\")\n",
    "print(f\"Validation size is now: {len(os.listdir(validfolderimgs))}\")\n",
    "print(f\"Test size is now: {len(os.listdir(testfolderimgs))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGyMZpYMEjiq"
   },
   "source": [
    "# Train a yolo model with the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4QscbvTJWed"
   },
   "outputs": [],
   "source": [
    "!rm -rf runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m-q_fftSUQ3D"
   },
   "outputs": [],
   "source": [
    "def clean_labels(split):\n",
    "    label_dir = f'/content/ds_final/{split}/labels'\n",
    "    if not os.path.exists(label_dir):\n",
    "        print(f\"Label directory for {split} not found.\")\n",
    "        return\n",
    "\n",
    "    for label_file in os.listdir(label_dir):\n",
    "        path = os.path.join(label_dir, label_file)\n",
    "        with open(path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        valid_lines = []\n",
    "        for line in lines:\n",
    "            try:\n",
    "                parts = list(map(float, line.strip().split()))\n",
    "                if len(parts) == 5:\n",
    "                    _, x, y, w, h = parts\n",
    "                    if all(0 <= val <= 1 for val in [x, y, w, h]):\n",
    "                        valid_lines.append(line)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing line in {label_file}: {line} -> {e}\")\n",
    "\n",
    "        if valid_lines:\n",
    "            with open(path, 'w') as f:\n",
    "                f.writelines(valid_lines)\n",
    "        else:\n",
    "            os.remove(path)\n",
    "            print(f\"‚ùå Removed invalid label file: {path}\")\n",
    "\n",
    "# Clean labels for all splits\n",
    "for split in ['train', 'test', 'valid']:\n",
    "    print(f\"üßπ Cleaning {split} labels...\")\n",
    "    clean_labels(split)\n",
    "\n",
    "print(\"‚úÖ Done cleaning labels!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6gOyra8VE0-0",
    "outputId": "84614c16-919f-4515-c2de-825b912d8d29"
   },
   "outputs": [],
   "source": [
    "!pip install ultralytics\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a YOLOv8 model (you can also use 'yolov8n.yaml', 'yolov8s.yaml', etc.)\n",
    "model = YOLO(\"yolov8l.yaml\")  # 'n' = nano, smallest model\n",
    "\n",
    "# Train the model\n",
    "model.train(\n",
    "    data=\"./ds_final/data.yaml\",\n",
    "    epochs=50,\n",
    "    imgsz=640,\n",
    "    batch=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TY83CIQaFcPO",
    "outputId": "bc4c0ca2-e550-408d-b8af-d069593cf480"
   },
   "outputs": [],
   "source": [
    "# Test the model\n",
    "#model.val(data=\"ds_final/data.yaml\", split=\"test\")\n",
    "\n",
    "# Load the best trained model\n",
    "model = YOLO(\"runs/detect/train/weights/best.pt\")\n",
    "\n",
    "# Run inference on test images\n",
    "results = model.predict(source=\"ds_final/test/images\", save=True, conf=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_x_9sjLiIwV"
   },
   "source": [
    "TODO:\n",
    "\n",
    "* Images are too cropped outside.\n",
    "* Truncate labels.\n",
    "* Not overlap too much.\n",
    "* Better segment objects?\n",
    "* Limit size of objects."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
